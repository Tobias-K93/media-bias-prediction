{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbnNCGUKd8pZ"
   },
   "source": [
    "## Media bias prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Jnt9ijkCvyn"
   },
   "source": [
    "### Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12488,
     "status": "ok",
     "timestamp": 1583836468506,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "lN-Hhw3-eTRW",
    "outputId": "c78b6fc8-983c-4fc4-8507-9b68288fee1a"
   },
   "outputs": [],
   "source": [
    "##### Importing packages\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-UVBve_eldq"
   },
   "outputs": [],
   "source": [
    "##### Choosing working directory\n",
    "#os.chdir('/content/gdrive/My Drive/thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9837,
     "status": "ok",
     "timestamp": 1583836468516,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "7nkk1WMJ0Ahn",
    "outputId": "4f89b52e-2865-4732-8d25-a7efba8d3a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "Is available\n"
     ]
    }
   ],
   "source": [
    "### Getting GPU type\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    print('Is available')\n",
    "else:\n",
    "    print('is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMdBnw8qCiK7"
   },
   "source": [
    "### Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JepmhMQOetOJ"
   },
   "outputs": [],
   "source": [
    "##### Loading tensors\n",
    "bias_train = torch.load('bias_train.pt')\n",
    "bias_val = torch.load('bias_val.pt')\n",
    "text_train = torch.load('contents_text_train.pt')\n",
    "text_val = torch.load('contents_text_val.pt')\n",
    "mask_train = torch.load('contents_mask_train.pt')\n",
    "mask_val = torch.load('contents_mask_val.pt')\n",
    "# text_train = torch.load('contents_text_source_removed_train.pt')\n",
    "# text_val = torch.load('contents_text_source_removed_val.pt')\n",
    "# mask_train = torch.load('contents_mask_source_removed_train.pt')\n",
    "# mask_val = torch.load('contents_mask_source_removed_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ3NLmdsewZz"
   },
   "outputs": [],
   "source": [
    "##### Creating training and validation sets for pytorch models\n",
    "train_set = TensorDataset(text_train, mask_train, bias_train)\n",
    "val_set = TensorDataset(text_val, mask_val, bias_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FBEyXfgBSkP"
   },
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyAtdhE_fTot"
   },
   "outputs": [],
   "source": [
    "##### Create Model class\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, droput_prob, bert_model_module, output_attentions=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.output_attentions = output_attentions\n",
    "\n",
    "        self.bert = bert_model_module\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier_layer = nn.Linear(hidden_size, num_labels) # The values are initialized from U(−sqrt(k),sqrt(k)), where k=1/in_features\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        # token_type_ids and position_ids are created automaticly \n",
    "        bert_out = self.bert(input_ids = text, attention_mask = mask)\n",
    "        # Choosing only CLS token output and apply linear layer + TanH \n",
    "        pooled_out = bert_out[1]\n",
    "        # Applying dropout\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        # Add classifier\n",
    "        out = self.classifier_layer(pooled_out)\n",
    "        # Saving attention layer outputs if set True\n",
    "        if self.output_attentions:\n",
    "          out = out, bert_out[2]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxRtMd4EBbpH"
   },
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxXWoN06fUde"
   },
   "outputs": [],
   "source": [
    "##### Function for training of 1 epoch\n",
    "\n",
    "def train_fct(train_set, batch_size, batch_feedback = 500, first_check = 10, mixed_precision = False):\n",
    "    start_time = time.time()\n",
    "    # Setting model to train mode (so dropout is applied)\n",
    "    model.train()\n",
    "    # creating iterable dataset devided into batches and shuffled\n",
    "    data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # tracking batches, loss, accuracy\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correctly_specified = 0\n",
    "    train_predicted_values = []\n",
    "    train_true_values = []\n",
    "    # looping over batches\n",
    "    for text, mask, label in data:\n",
    "        # sending tensors to GPU\n",
    "        text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "        # clearing gradients\n",
    "        optimizer.zero_grad()\n",
    "        # run through model\n",
    "        output = model(text, mask)\n",
    "        # calculating loss\n",
    "        loss = loss_fct(output, label)\n",
    "        # backpropagation\n",
    "        if mixed_precision:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # updating weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss and metrices messures\n",
    "        train_loss += loss.item()\n",
    "        train_correctly_specified += (output.argmax(1) == label).sum().item()\n",
    "        \n",
    "        train_predicted_values.append(output.argmax(1))\n",
    "        train_true_values.append(label)\n",
    "        \n",
    "        # adding to batchcounter\n",
    "        batch_counter += 1\n",
    "\n",
    "        if (batch_counter % batch_feedback == 0) or (batch_counter == first_check):\n",
    "            time_so_far = time.time() - start_time\n",
    "            minutes = time_so_far // 60\n",
    "            seconds = time_so_far % 60\n",
    "            average_progress_loss = train_loss/batch_counter\n",
    "            progress_acc = train_correctly_specified/(batch_counter*batch_size)\n",
    "            print('---------------------------------------')\n",
    "            print('%d batches done after %d min %d sec'%(batch_counter,minutes,seconds))\n",
    "            print('---------------------------------------')\n",
    "            print('loss: %.4f \\t|\\tacc: %.4f'%(average_progress_loss, progress_acc))\n",
    "            print('---------------------------------------')\n",
    "    \n",
    "    # loss\n",
    "    average_total_loss = train_loss/(len(train_set)/batch_size)\n",
    "    # accuracy\n",
    "    total_accuracy = train_correctly_specified/len(train_set) \n",
    "    # Predicted and true values\n",
    "    train_predicted_values = torch.cat(train_predicted_values).cpu().numpy()\n",
    "    train_true_values = torch.cat(train_true_values).cpu().numpy()\n",
    "    # Precision\n",
    "    train_precision = precision_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    train_recall = recall_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    train_f1_score = f1_score(train_true_values, train_predicted_values, average='macro')\n",
    "    \n",
    "    return average_total_loss, total_accuracy, train_precision, train_recall, train_f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ur0u4KrbBeSG"
   },
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "705wLtc2fZQP"
   },
   "outputs": [],
   "source": [
    "##### Function for validation after 1 epoch of training\n",
    "\n",
    "def val_fct(val_set, batch_size):\n",
    "    # Setting model to evaluation mode (dropout is not applied)\n",
    "    model.eval()\n",
    "    # creating iterable dataset devided into batches, not shuffeled\n",
    "    data = DataLoader(val_set, batch_size = batch_size)\n",
    "    # setting up loss and accuracy variables\n",
    "    val_loss = 0\n",
    "    #val_correctly_specified = 0\n",
    "    val_predicted_values = []\n",
    "    val_true_values = []\n",
    "    # looping over batches\n",
    "    for text, mask, label in data:\n",
    "        text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "        # no gradient calculation during validation\n",
    "        with torch.no_grad():\n",
    "            output = model(text,mask)\n",
    "            loss = loss_fct(output, label)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            #val_correctly_specified += (output.argmax(1) == label).sum().item()\n",
    "            val_predicted_values.append(output.argmax(1))\n",
    "            val_true_values.append(label)\n",
    "    \n",
    "    # loss\n",
    "    average_val_loss = val_loss/(len(val_set)/batch_size)\n",
    "    # true and predicted values\n",
    "    val_predicted_values = torch.cat(val_predicted_values).cpu().numpy()\n",
    "    val_true_values = torch.cat(val_true_values).cpu().numpy()\n",
    "    # Accuracy\n",
    "    val_accuracy = (val_predicted_values==val_true_values).sum().item()/len(val_set) #val_correctly_specified/len(val_set)\n",
    "    # Precision\n",
    "    val_precision = precision_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    val_recall = recall_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    val_f1_score = f1_score(val_true_values, val_predicted_values, average='macro')\n",
    "\n",
    "    return average_val_loss, val_accuracy, val_precision, val_recall, val_f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1_fhMMKBk7H"
   },
   "source": [
    "### Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhkDo2FbfdaV"
   },
   "outputs": [],
   "source": [
    "##### Loading Bert \n",
    "BertModel = transformers.BertModel\n",
    "# BertTokenizer = transformers.BertTokenizer\n",
    "# bert_pretrained_weights = 'bert-base-uncased'\n",
    "\n",
    "### Device to run model on, either GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Model inputs\n",
    "hidden_size = 768\n",
    "num_labels = 3\n",
    "dropout_prob = 0.1\n",
    "### Batch size for training and validation\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ0MZ1aufnm0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "##### Initilize and configure Bert\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "\n",
    "##### Initilize model\n",
    "model = Model(hidden_size, num_labels, dropout_prob, bert_model).to(device)\n",
    "\n",
    "### Optimizer, choosing learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00002)\n",
    "\n",
    "### Applying mixed precision to speed up model training\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "### Loss function\n",
    "loss_fct = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erzvb_j-BthO"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12026858,
     "status": "ok",
     "timestamp": 1583761704500,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "u6EPprzofre4",
    "outputId": "7351c581-fb46-4784-fc4d-9841f8c7d759",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "---------------------------------------\n",
      "100 batches done after 1 min 14 sec\n",
      "---------------------------------------\n",
      "loss: 0.9439 \t|\tacc: 0.5175\n",
      "---------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "---------------------------------------\n",
      "1000 batches done after 12 min 25 sec\n",
      "---------------------------------------\n",
      "loss: 0.7179 \t|\tacc: 0.6665\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "2000 batches done after 24 min 51 sec\n",
      "---------------------------------------\n",
      "loss: 0.6093 \t|\tacc: 0.7255\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "3000 batches done after 37 min 17 sec\n",
      "---------------------------------------\n",
      "loss: 0.5484 \t|\tacc: 0.7570\n",
      "---------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "---------------------------------------\n",
      "4000 batches done after 49 min 43 sec\n",
      "---------------------------------------\n",
      "loss: 0.5126 \t|\tacc: 0.7765\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "5000 batches done after 62 min 9 sec\n",
      "---------------------------------------\n",
      "loss: 0.4849 \t|\tacc: 0.7906\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "6000 batches done after 74 min 35 sec\n",
      "---------------------------------------\n",
      "loss: 0.4653 \t|\tacc: 0.8003\n",
      "---------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "---------------------------------------\n",
      "7000 batches done after 87 min 2 sec\n",
      "---------------------------------------\n",
      "loss: 0.4492 \t|\tacc: 0.8084\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "8000 batches done after 99 min 27 sec\n",
      "---------------------------------------\n",
      "loss: 0.4367 \t|\tacc: 0.8147\n",
      "---------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "---------------------------------------\n",
      "9000 batches done after 111 min 53 sec\n",
      "---------------------------------------\n",
      "loss: 0.4247 \t|\tacc: 0.8204\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "10000 batches done after 124 min 18 sec\n",
      "---------------------------------------\n",
      "loss: 0.4143 \t|\tacc: 0.8253\n",
      "---------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "---------------------------------------\n",
      "11000 batches done after 136 min 44 sec\n",
      "---------------------------------------\n",
      "loss: 0.4061 \t|\tacc: 0.8293\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "12000 batches done after 149 min 8 sec\n",
      "---------------------------------------\n",
      "loss: 0.3983 \t|\tacc: 0.8331\n",
      "---------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "---------------------------------------\n",
      "13000 batches done after 161 min 33 sec\n",
      "---------------------------------------\n",
      "loss: 0.3905 \t|\tacc: 0.8367\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "14000 batches done after 173 min 59 sec\n",
      "---------------------------------------\n",
      "loss: 0.3839 \t|\tacc: 0.8400\n",
      "---------------------------------------\n",
      "Epoch: 1 took 188 min, 17 sec\n",
      "(Training)   Loss: 0.3808  |  Acc: 0.8414  |  F1: 0.8347\n",
      "(Validation) Loss: 0.2818  |  Acc: 0.8877  |  F1: 0.8821\n"
     ]
    }
   ],
   "source": [
    "### Choosing number of epochs to train\n",
    "num_epochs = 1\n",
    "\n",
    "### Lists to save metrices\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "train_precision_list = []\n",
    "val_precision_list = []\n",
    "train_recall_list = []\n",
    "val_recall_list = []\n",
    "train_f1_list = []\n",
    "val_f1_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    # Training for 1 epoch\n",
    "    train_loss, train_acc, train_precision, train_recall, train_f1_score = train_fct(train_set, \n",
    "                                                                                     batch_size, \n",
    "                                                                                     batch_feedback=2000, \n",
    "                                                                                     first_check=100, \n",
    "                                                                                     mixed_precision = True)\n",
    "    # Validation\n",
    "    val_loss, val_acc, val_precision, val_recall, val_f1_score = val_fct(val_set, batch_size)\n",
    "    # saving metrices\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    train_precision_list.append(train_precision)\n",
    "    train_recall_list.append(train_recall)\n",
    "    val_precision_list.append(val_precision)\n",
    "    val_recall_list.append(val_recall)\n",
    "    train_f1_list.append(train_f1_score)\n",
    "    val_f1_list.append(val_f1_score)\n",
    "\n",
    "    end = time.time() - epoch_start_time\n",
    "    minutes = end // 60\n",
    "    seconds = end % 60\n",
    "\n",
    "    print('Epoch: %d took %d min, %d sec' %((epoch + 1), minutes, seconds))\n",
    "    print('(Training)   Loss: %.4f  |  Acc: %.4f  |  F1: %.4f' %(train_loss, train_acc, train_f1_score))\n",
    "    print('(Validation) Loss: %.4f  |  Acc: %.4f  |  F1: %.4f' %(val_loss, val_acc, val_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the GitHub Discussion regarding gradient overflow: \"Occasionally seeing a message like “overflow detected, skipping step, reducing loss scale” is normal behavior with dynamic loss scaling, and it usually happens in the first few iterations because Amp begins by trying a high loss scale.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1322,
     "status": "ok",
     "timestamp": 1583761886789,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "u5RCvz5dEwpZ",
    "outputId": "dea7ac62-86ee-4981-f61d-b6232c3f9142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Loss \t\t Accuracy \t\t Precision \t\tRecall\t\t F1 Score\n",
      "[0.38075399172641655] [0.8413855224369473] [0.8365040967251182] [0.8332343787158448] [0.8346977454141338]\n",
      "[0.28176233467110734] [0.8876706661150187] [0.8709227602050201] [0.8956481279037334] [0.8821069317241093]\n"
     ]
    }
   ],
   "source": [
    "### Results\n",
    "print('\\t Loss \\t\\t Accuracy \\t\\t Precision \\t\\tRecall\\t\\t F1 Score')\n",
    "print(train_loss_list, train_acc_list, train_precision_list, train_recall_list, train_f1_list)\n",
    "print(val_loss_list, val_acc_list, val_precision_list, val_recall_list, val_f1_list)\n",
    "\n",
    "##### Colab\n",
    "### source removed, lr 0.00001\n",
    "# Epoch 1 validation: 0.020134240149200136 0.8713280926768722 0.8558285658209931 0.8843389771463251 0.8677143693730516\n",
    "\n",
    "# Epoch 2 training: [0.01456093337804093] [0.9087265330046373] [0.9054350576361471] [0.9043001074317857] [0.904841879952636]\n",
    "# Epoch 2 validation: [0.0153729185697558] [0.9068749138049924] [0.9036977666780507] [0.9054265988915527] [0.9044335065693452]\n",
    "# Epoch 3 training:[0.16084885144930583] [0.9383738169531264] [0.9353458111355638] [0.935262652754551] [0.9352988023156548]\n",
    "# Epoch 3 validation: [0.24885340416995333] [0.9111846641842505] [0.900652469742982] [0.9150473003246713] [0.9074495190199744]\n",
    "# Epoch 4 training: [0.11022477680434647] [0.9591945799644871] [0.9561934008096636] [0.9566272381119848] [0.9564094295953569]\n",
    "# Epoch 4 validation:[0.27568220538593885] [0.9179768307819611] [0.9112980287133338] [0.9177220353054366] [0.9143656261644293]\n",
    "# Epoch 5 training:[0.07788220056710543] [0.9718827038116089] [0.9690285752575499] [0.9698152477910802] [0.9694206627426398]\n",
    "# Epoch 5 validation:[0.2727730815459278] [0.9158047165908151] [0.9057021897800093] [0.9187037798215177] [0.9118923921686556]\n",
    "\n",
    "### train without, val with source in text\n",
    "# Epoch 1 training: [0.3971200964916292] [0.8324641853569397] [0.828299376800027] [0.8212768603546242] [0.8245316879281454]\n",
    "# Epoch 1 validation:[0.4229980396116344] [0.8383671217763067] [0.8349940151450616] [0.828457246274632] [0.8306645900549009]\n",
    "\n",
    "### lr=0.00002\n",
    "# Epoch 1 training: [0.37530845116653017] [0.8436223214439637] [0.8381188959089543] [0.8349232930591916] [0.8363652803688476]\n",
    "# Epoch 2 validation: [0.2957536905301125] [0.8842918218176803] [0.8621790465931204] [0.8923710220376032] [0.875162090160778]\n",
    "\n",
    "##### GCP\n",
    "### lr=0.00002, T4, Apex\n",
    "# Epoch 1 training: [0.38075399172641655] [0.8413855224369473] [0.8365040967251182] [0.8332343787158448] [0.8346977454141338]\n",
    "# Epoch 1 validation: [0.28176233467110734] [0.8876706661150187] [0.8709227602050201] [0.8956481279037334] [0.8821069317241093]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhQptbzYB3Nm"
   },
   "source": [
    "### Saving/Loading Model\n",
    "\n",
    "#### Without Apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blkg3vF2fr7J"
   },
   "outputs": [],
   "source": [
    "### Saving model weights\n",
    "# torch.save(model.state_dict(), 'weights/model_weights_epoch1_lr0.00002.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21113,
     "status": "ok",
     "timestamp": 1583419030876,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "VqEoHEiKhGja",
    "outputId": "aef73810-3b9a-43ea-a385-c92105af2d92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading model weights\n",
    "# model.load_state_dict(torch.load('weights/model_weights_train_no_source_val_with_source_epoch1.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving checkpoint\n",
    "checkpoint = {'model': model.state_dict(),\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'amp': amp.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'weights/amp_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restoring\n",
    "checkpoint = torch.load('weights/amp_checkpoint.pt')\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "amp.load_state_dict(checkpoint['amp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping instance\n",
    "! gcloud compute instances stop t4-instance --zone=europe-west4-c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-xja4WkB-MR"
   },
   "source": [
    "### Testing validation adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSZ3rdNWg2fF"
   },
   "outputs": [],
   "source": [
    "###### Testing different things in validation\n",
    "model.eval()\n",
    "data = DataLoader(val_set, batch_size = batch_size)\n",
    "test_loss = 0\n",
    "test_predicted_values = []\n",
    "test_true_values = []\n",
    "\n",
    "for text, mask, label in data:\n",
    "    text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(text,mask)\n",
    "        loss = loss_fct(output, label)\n",
    "\n",
    "        try:\n",
    "          test_probabilities = torch.cat([test_probabilities, output])\n",
    "        except NameError:\n",
    "          test_probabilities = output\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_predicted_values.append(output.argmax(1))\n",
    "        test_true_values.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P84oBw0znY3O"
   },
   "outputs": [],
   "source": [
    "##### For Testing of validation\n",
    "# loss\n",
    "average_test_loss = test_loss/(len(val_set)/batch_size)\n",
    "# true and predicted values\n",
    "#test_predicted_values = torch.cat(test_predicted_values).cpu().numpy()\n",
    "#test_true_values = torch.cat(test_true_values).cpu().numpy()\n",
    "# Accuracy\n",
    "test_accuracy = (test_predicted_values==test_true_values).sum().item()/len(val_set) #val_correctly_specified/len(val_set)\n",
    "# Precision\n",
    "test_precision = precision_score(test_true_values, test_predicted_values, average='macro')\n",
    "# Recall\n",
    "test_recall = recall_score(test_true_values, test_predicted_values, average='macro')\n",
    "# F1 score\n",
    "test_f1_score = f1_score(test_true_values, test_predicted_values, average='macro')\n",
    "\n",
    "test_probabilities = F.softmax(test_probabilities, dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1582903236325,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "hFfz2aA0O6xX",
    "outputId": "c245b6f8-df04-406f-9ee2-2697906a83d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26215972163737716 0.899669011170873 0.895641877338377\n"
     ]
    }
   ],
   "source": [
    "print(average_test_loss, test_accuracy, test_f1_score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CW-RalevCEOy"
   },
   "source": [
    "### Exploring and testing model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a11d78d55d0c4cf8997ee5f00770a18c",
      "2c0917ab40df46108ed42946d1ce8197",
      "30cf4cc096c94604b20b0ec140d55ee6",
      "097863cb120d4f468a90d0762ec8db4d",
      "8a1dc8ced72b42a3acbb5b3fa8b00cae",
      "40d47fd84ae74380b8b5a871e0a3525d",
      "98bbf6f7b90e40248b5edde72762ffab",
      "b41ffac0c10d4e8688172566575a22af"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1065,
     "status": "ok",
     "timestamp": 1583167426394,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "-aVrbVDhDJws",
    "outputId": "fcaeedcb-1161-4a74-a362-f2a69722c2e7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11d78d55d0c4cf8997ee5f00770a18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading and initilizing tokenizer\n",
    "BertTokenizer = transformers.BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S52AlDeBiSLf"
   },
   "source": [
    "#### Having a look at attention layers\n",
    "For this to work, output_attentions=True needs to be chosen during loading of pretrained Bert weights and when initilizing own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-l17sTWozsnw"
   },
   "outputs": [],
   "source": [
    "### Load model weights with configuration to output attentions\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True) \n",
    "\n",
    "##### Initilize model version that handles attention outptu\n",
    "model = Model(hidden_size, num_labels, dropout_prob, bert_model, output_attentions=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKpXjnc8iZE_"
   },
   "outputs": [],
   "source": [
    "##### Using code from val_fct and adjust it to attention output\n",
    "model.eval()\n",
    "data = DataLoader(val_set, batch_size = batch_size)\n",
    "val_loss = 0\n",
    "val_predicted_values = []\n",
    "val_true_values = []\n",
    "\n",
    "for text, mask, label in data:\n",
    "    text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # model outputs predictions and attention layers \n",
    "        output = model(text,mask)\n",
    "        model_output = output[0]\n",
    "        attentions = output[1]\n",
    "\n",
    "        loss = loss_fct(model_output, label)\n",
    "\n",
    "        # try:\n",
    "        #   val_probabilities = torch.cat([test_probabilities, output])\n",
    "        # except NameError:\n",
    "        #   val_probabilities = output\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_predicted_values.append(model_output.argmax(1))\n",
    "        val_true_values.append(label)\n",
    "    \n",
    "    ### Get only first batch of attentions\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdgFhJYpIXVN"
   },
   "outputs": [],
   "source": [
    "### Get input tensor with tokens\n",
    "att_tokens = np.array(bert_tokenizer.convert_ids_to_tokens(text[0,:].cpu()))\n",
    "### extract single attention weight matrices\n",
    "attentions[0][0,0,:,:].cpu() # [layer?][batch,head,sequence,sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYiQ-xabDCny"
   },
   "source": [
    "#### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBzeSoFGnL6x"
   },
   "outputs": [],
   "source": [
    "# Show digits without scientific display\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymVNZbQerf4W"
   },
   "outputs": [],
   "source": [
    "# Loading Sources of validation set\n",
    "source_val = np.load('source_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6aOgbWlweze"
   },
   "outputs": [],
   "source": [
    "# Dictionaries for source convertion\n",
    "source_dict = {'Addicting Info': 0, 'Al Jazeera': 1, 'Alternet': 2, 'BBC': 3, 'Bearing Arms': 4, 'Bipartisan Report': 5, 'Breitbart': 6, 'Business Insider': 7, 'CNBC': 8, 'CNN': 9, 'CNS News': 10, 'Crooks and Liars': 11, 'DC Gazette': 12, 'Daily Beast': 13, 'Daily Kos': 14, 'Daily Mail': 15, 'Daily Signal': 16, 'Daily Stormer': 17, 'Drudge Report': 18, 'Feministing Blog': 19, 'FiveThirtyEight': 20, 'Foreign Policy': 21, 'Forward Progessives': 22, 'Fox News': 23, 'Freedom Daily': 24, 'FrontPage Magazine': 25, 'Hot Air': 26, 'Infowars': 27, 'Investors Business Daily': 28, 'LewRockwell': 29, 'MSNBC': 30, 'Media Matters for America': 31, 'MotherJones': 32, 'NPR': 33, 'National Review': 34, 'New York Daily News': 35, 'New York Post': 36, 'New Yorker': 37, 'Newswars': 38, 'Newsweek': 39, 'PBS': 40, 'Palmer Report': 41, 'Pamela Geller Report': 42, 'Pink News UK': 43, 'Politico': 44, 'Politicus USA': 45, 'Pravada Report': 46, 'Raw Story': 47, 'Real Clear Politics': 48, 'RedState': 49, 'Reuters': 50, 'Salon': 51, 'Shadow Proof': 52, 'Shareblue': 53, 'Slate': 54, 'Talking Points Memo': 55, 'Telesur TV': 56, 'The D.C. Clothesline': 57, 'The Daily Caller': 58, 'The Daily Express': 59, 'The Daily Mirror': 60, 'The Daily Record': 61, 'The Duran': 62, 'The Gateway Pundit': 63, 'The Hill': 64, 'The Huffington Post': 65, 'The Intercept': 66, 'The Michelle Malkin Blog': 67, 'The Political Insider': 68, 'The Right Scoop': 69, 'The Sun': 70, 'The Washington Examiner': 71, 'TheBlaze': 72, 'ThinkProgress': 73, 'True Activist': 74, 'USA Today': 75, 'Vox': 76, 'Washington Monthly': 77, 'Western Journal': 78, 'Yahoo News': 79}\n",
    "source_dict_inverse = {0: 'Addicting Info', 1: 'Al Jazeera', 2: 'Alternet', 3: 'BBC', 4: 'Bearing Arms', 5: 'Bipartisan Report', 6: 'Breitbart', 7: 'Business Insider', 8: 'CNBC', 9: 'CNN', 10: 'CNS News', 11: 'Crooks and Liars', 12: 'DC Gazette', 13: 'Daily Beast', 14: 'Daily Kos', 15: 'Daily Mail', 16: 'Daily Signal', 17: 'Daily Stormer', 18: 'Drudge Report', 19: 'Feministing Blog', 20: 'FiveThirtyEight', 21: 'Foreign Policy', 22: 'Forward Progessives', 23: 'Fox News', 24: 'Freedom Daily', 25: 'FrontPage Magazine', 26: 'Hot Air', 27: 'Infowars', 28: 'Investors Business Daily', 29: 'LewRockwell', 30: 'MSNBC', 31: 'Media Matters for America', 32: 'MotherJones', 33: 'NPR', 34: 'National Review', 35: 'New York Daily News', 36: 'New York Post', 37: 'New Yorker', 38: 'Newswars', 39: 'Newsweek', 40: 'PBS', 41: 'Palmer Report', 42: 'Pamela Geller Report', 43: 'Pink News UK', 44: 'Politico', 45: 'Politicus USA', 46: 'Pravada Report', 47: 'Raw Story', 48: 'Real Clear Politics', 49: 'RedState', 50: 'Reuters', 51: 'Salon', 52: 'Shadow Proof', 53: 'Shareblue', 54: 'Slate', 55: 'Talking Points Memo', 56: 'Telesur TV', 57: 'The D.C. Clothesline', 58: 'The Daily Caller', 59: 'The Daily Express', 60: 'The Daily Mirror', 61: 'The Daily Record', 62: 'The Duran', 63: 'The Gateway Pundit', 64: 'The Hill', 65: 'The Huffington Post', 66: 'The Intercept', 67: 'The Michelle Malkin Blog', 68: 'The Political Insider', 69: 'The Right Scoop', 70: 'The Sun', 71: 'The Washington Examiner', 72: 'TheBlaze', 73: 'ThinkProgress', 74: 'True Activist', 75: 'USA Today', 76: 'Vox', 77: 'Washington Monthly', 78: 'Western Journal', 79: 'Yahoo News'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHG7GyJyABFZ"
   },
   "outputs": [],
   "source": [
    "# Convertion of texts from ids to tokens\n",
    "bert_tokenizer.convert_ids_to_tokens('Sequence of Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9FmuD-wDQoj"
   },
   "source": [
    "#### Predicting on other articles (copy past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EECXrBML3Ndt"
   },
   "outputs": [],
   "source": [
    "##### Testing other articles\n",
    "new_article = 'Donald Trump once lied again! It is incredible how stupid that man is and the similarly stupid GOP keeps backing him. Republican Senator Lindsey Graham said \"The Democrats have no idea what they are talking about. '\n",
    "\n",
    "# encode article to get mask and encoded text in form of a dictionary\n",
    "new_article_dict = bert_tokenizer.encode_plus(new_article, max_length= 500, pad_to_max_length=True, return_tensors='pt', return_token_type_ids=False)\n",
    "new_text = new_article_dict['input_ids'].to(device)\n",
    "new_mask = new_article_dict['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1582997740535,
     "user": {
      "displayName": "Kre Tob",
      "photoUrl": "",
      "userId": "01995262477345906308"
     },
     "user_tz": -60
    },
    "id": "2HJtLgSz4hT9",
    "outputId": "3d84d12f-5ae0-49ed-f176-7ea42046814f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left: 95.33% center: 0.30% right: 4.36%\n"
     ]
    }
   ],
   "source": [
    "# Apply model to new article\n",
    "with torch.no_grad():\n",
    "        output = model(new_text, new_mask)\n",
    "\n",
    "probabilities = F.softmax(output, dim=1).cpu().numpy().reshape(-1)*100\n",
    "print('left: %.2f%% center: %.2f%% right: %.2f%%'% (probabilities[0], probabilities[1], probabilities[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJ4CFlaC_Ti9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNqtZwBVlatUXXt73q55Owq",
   "machine_shape": "hm",
   "name": "thesis_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "097863cb120d4f468a90d0762ec8db4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b41ffac0c10d4e8688172566575a22af",
      "placeholder": "​",
      "style": "IPY_MODEL_98bbf6f7b90e40248b5edde72762ffab",
      "value": "100% 232k/232k [00:00&lt;00:00, 1.20MB/s]"
     }
    },
    "2c0917ab40df46108ed42946d1ce8197": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30cf4cc096c94604b20b0ec140d55ee6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40d47fd84ae74380b8b5a871e0a3525d",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a1dc8ced72b42a3acbb5b3fa8b00cae",
      "value": 231508
     }
    },
    "40d47fd84ae74380b8b5a871e0a3525d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a1dc8ced72b42a3acbb5b3fa8b00cae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "98bbf6f7b90e40248b5edde72762ffab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a11d78d55d0c4cf8997ee5f00770a18c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30cf4cc096c94604b20b0ec140d55ee6",
       "IPY_MODEL_097863cb120d4f468a90d0762ec8db4d"
      ],
      "layout": "IPY_MODEL_2c0917ab40df46108ed42946d1ce8197"
     }
    },
    "b41ffac0c10d4e8688172566575a22af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
