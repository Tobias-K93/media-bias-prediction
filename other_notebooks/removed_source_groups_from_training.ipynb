{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on sources it was not trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to media-bias-prediction repository \n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "os.chdir(os.path.join(repo_path, 'other_notebooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(repo_path, 'data_preparation','allsides_data'))\n",
    "\n",
    "bias_test = torch.load('allsides_bias_test.pt')\n",
    "text_test = torch.load('allsides_contents_text_test.pt')\n",
    "mask_test = torch.load('allsides_contents_mask_test.pt')\n",
    "\n",
    "os.chdir(os.path.join(repo_path, 'other_notebooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(repo_path, 'data_preparation','allsides_data'))\n",
    "\n",
    "allsides_source_test = np.load('allsides_source_test.npy', allow_pickle=True).flatten()\n",
    "\n",
    "# sources to be removed:\n",
    "wrongly_labeled = ['RightWingWatch']\n",
    "news_aggregators = ['Drudge Report', 'Real Clear Politics', 'Yahoo News', 'RightWingWatch'] \n",
    "tabloids = ['New York Daily News', 'Daily Mail', 'New York Post'] \n",
    "unwanted_sources = wrongly_labeled + news_aggregators + tabloids\n",
    "# creating boolean array to mark unwanted sources\n",
    "boolean_array_test = np.full((len(allsides_source_test), ), False)\n",
    "\n",
    "for source in unwanted_sources:\n",
    "    boolean_array_test += allsides_source_test==source \n",
    "# boolean to remove aggregators\n",
    "inverted_boolean_array_test = np.invert(boolean_array_test)\n",
    "\n",
    "# bias\n",
    "bias_test = bias_test[inverted_boolean_array_test]\n",
    "\n",
    "# text and masks\n",
    "text_test = text_test[inverted_boolean_array_test]\n",
    "mask_test = mask_test[inverted_boolean_array_test]\n",
    "\n",
    "# sources\n",
    "allsides_source_test = allsides_source_test[inverted_boolean_array_test]\n",
    "\n",
    "os.chdir(os.path.join(repo_path, 'other_notebooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TensorDataset(text_test, mask_test, bias_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, droput_prob, bert_model_module, output_attentions=False, pooled_output = True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.output_attentions = output_attentions\n",
    "        self.pooled_output = pooled_output\n",
    "\n",
    "        self.bert = bert_model_module\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.linear = nn.Linear(hidden_size,hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.classifier_layer = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, text, mask):\n",
    "        # token_type_ids and position_ids are created automaticly \n",
    "        bert_out = self.bert(input_ids = text, attention_mask = mask)\n",
    "        \n",
    "        if self.pooled_output:\n",
    "            # Choosing only CLS token output and apply linear layer + TanH \n",
    "            pooled_out = bert_out[1]\n",
    "            # Applying dropout\n",
    "            pooled_out = self.dropout(pooled_out)\n",
    "\n",
    "            out = self.classifier_layer(pooled_out)\n",
    "        else:\n",
    "            ### Last Layer average\n",
    "            # summing up over sequence lenght and devide by unmasked sequence length \n",
    "            # resulting in tensor with shape (batch_size,hidden_size)\n",
    "            last_layer = torch.sum(bert_out[0], dim=1)/torch.sum(mask,dim=1).reshape([len(mask),1])\n",
    "            last_layer = self.tanh(self.linear(last_layer))\n",
    "            last_layer = self.dropout(last_layer)\n",
    "            out = self.classifier_layer(last_layer)\n",
    "               \n",
    "        # Saving attention layer outputs if set True\n",
    "        if self.output_attentions:\n",
    "            out = out, bert_out[2]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading Bert \n",
    "BertModel = transformers.BertModel\n",
    "\n",
    "### Device to run model on, either GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Model inputs\n",
    "hidden_size = 768\n",
    "num_labels = 5 \n",
    "dropout_prob = 0.1\n",
    "\n",
    "mixed_precision = True\n",
    "\n",
    "### Hyperparameters\n",
    "batch_size = 16 \n",
    "learning_rate = 2e-5\n",
    "\n",
    "##### Initilize and configure Bert\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "\n",
    "##### Initilize model \n",
    "model = Model(hidden_size, num_labels, dropout_prob, bert_model, pooled_output=True).to(device)\n",
    "### Applying mixed precision to speed up model inference \n",
    "if mixed_precision:\n",
    "    model = amp.initialize(model, opt_level=\"O1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fct():\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = DataLoader(test_set, batch_size=batch_size)\n",
    "    test_loss = 0\n",
    "    test_predicted_values = []\n",
    "\n",
    "    number_of_batches = len(test_set)//batch_size\n",
    "    update = 500\n",
    "    batch_counter = 0\n",
    "\n",
    "    for text, mask, label in data:\n",
    "        text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(text,mask)\n",
    "            test_predicted_values.append(output.argmax(1))\n",
    "\n",
    "        batch_counter += 1\n",
    "        if (batch_counter % update == 0) or (batch_counter == 100):\n",
    "            update_time = time.time() - start_time\n",
    "            minutes = int(update_time // 60)\n",
    "            seconds = round(update_time % 60)\n",
    "            print(f'{batch_counter:5} of {number_of_batches} batches done after {minutes:2} min {seconds:2} sec ')\n",
    "            print('-----------------------------------------------')\n",
    "    \n",
    "    print('all batches done')\n",
    "    print('-----------------------------------------------')\n",
    "    test_predicted_values = torch.cat(test_predicted_values).cpu().numpy()\n",
    "    test_true_values = bias_test.numpy()\n",
    "    \n",
    "    return test_predicted_values, test_true_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_group = ['Daily Kos','Washington Monthly', 'FiveThirtyEight',                \n",
    "               'The Washington Examiner', 'FrontPage Magazine']                    \n",
    "large_group = ['Politicus USA', 'ABC News', 'Reuters', 'Fox News', 'CNS News']\n",
    "\n",
    "for group_type in ['small', 'large']:\n",
    "    for model_weights in ['with_sources', 'without_sources']:\n",
    "        for run in range(1,4):\n",
    "            ##### Initilize and configure Bert\n",
    "            bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "            ##### Initilize model \n",
    "            model = Model(hidden_size, num_labels, dropout_prob, bert_model, pooled_output=True).to(device)\n",
    "            ### Applying mixed precision to speed up model inference \n",
    "            if mixed_precision:\n",
    "                model = amp.initialize(model, opt_level=\"O1\", verbosity=0)\n",
    "\n",
    "            ### Load model weights\n",
    "            if model_weights=='with_sources':\n",
    "                checkpoint = torch.load(os.path.join(repo_path, 'deep_learning_models', 'weights', f'amp_checkpoint_allsides_aggregators_tabloids_duplicates_removed_rerun_{run}_epoch3.pt')) \n",
    "            else:\n",
    "                checkpoint = torch.load(os.path.join(repo_path, 'deep_learning_models', 'weights', f'amp_checkpoint_allsides_robustness_check_{group_type}_rerun_{run}_epoch3.pt')) \n",
    "\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            \n",
    "            ### make predictions\n",
    "            print('+++++++++++++++++++++++++++++++++++++++++++++++') \n",
    "            print(f' predicting on {group_type} group {model_weights} run {run}')\n",
    "            print('+++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "                   \n",
    "            test_predicted_values, test_true_values = prediction_fct()\n",
    "            \n",
    "            # scores of removed sources\n",
    "            if group_type =='small':\n",
    "                removed_sources = small_group\n",
    "            else:\n",
    "                removed_sources = large_group\n",
    "                \n",
    "            removed_sources_scores = []\n",
    "            for i,source in enumerate(removed_sources):\n",
    "                pred = test_predicted_values[allsides_source_test==source]\n",
    "                true = test_true_values[allsides_source_test==source]\n",
    "\n",
    "                removed_sources_accuracy = (pred==true).sum()/len(pred)\n",
    "\n",
    "                removed_sources_scores.append([removed_sources_accuracy, len(pred)])\n",
    "\n",
    "            # scores of remaining sources\n",
    "            kept_sources_boolean = (allsides_source_test!=removed_sources[0])&(allsides_source_test!=removed_sources[1])&(allsides_source_test!=removed_sources[2])&(allsides_source_test!=removed_sources[3])&(allsides_source_test!=removed_sources[4])\n",
    "\n",
    "            kept_sources_pred = test_predicted_values[kept_sources_boolean]\n",
    "            kept_sources_true = test_true_values[kept_sources_boolean]\n",
    "\n",
    "            kept_sources_accuracy = (kept_sources_pred==kept_sources_true).sum()/len(kept_sources_pred)\n",
    "\n",
    "            removed_sources_scores.append([kept_sources_accuracy, len(kept_sources_pred)])\n",
    "\n",
    "            removed_sources_scores = pd.DataFrame(np.array(removed_sources_scores).transpose() , columns=removed_sources+['remaining'], index=['Acc', 'Frequency' ]).round(4)\n",
    "            print(removed_sources_scores)\n",
    "            removed_sources_scores.to_csv(f'scores/accuracy_scores_{group_type}_{model_weights}_run_{run}.csv', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
