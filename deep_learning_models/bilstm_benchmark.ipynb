{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "Is available\n"
     ]
    }
   ],
   "source": [
    "### Getting GPU type\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    print('Is available')\n",
    "else:\n",
    "    print('is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading tensors \n",
    "bias_train = torch.load('allsides/allsides_bias_train.pt')\n",
    "bias_val = torch.load('allsides/allsides_bias_val.pt')\n",
    "bias_test = torch.load('allsides/allsides_bias_test.pt')\n",
    "\n",
    "# text_train = torch.load('allsides/allsides_contents_text_train.pt')\n",
    "text_val = torch.load('allsides/allsides_contents_text_val.pt')\n",
    "text_test = torch.load('allsides/allsides_contents_text_test.pt')\n",
    "\n",
    "# allsides duplicates removed train\n",
    "text_train = torch.load('allsides/allsides_duplicates_removed_contents_text_train.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing news aggregators (and tabloids) from tensors \n",
    "allsides_source_train = np.load('allsides/allsides_source_train.npy', allow_pickle=True).flatten()\n",
    "allsides_source_val = np.load('allsides/allsides_source_val.npy', allow_pickle=True).flatten()\n",
    "allsides_source_test = np.load('allsides/allsides_source_test.npy', allow_pickle=True).flatten()\n",
    "\n",
    "# sources to be removed:\n",
    "wrongly_labeled = ['RightWingWatch']\n",
    "news_aggregators = ['Drudge Report', 'Real Clear Politics', 'Yahoo News', 'RightWingWatch'] \n",
    "tabloids = ['New York Daily News', 'Daily Mail', 'New York Post'] \n",
    "unwanted_sources = wrongly_labeled + news_aggregators + tabloids\n",
    "# creating boolean array to mark unwanted sources\n",
    "boolean_array_train = np.full((len(allsides_source_train), ), False)\n",
    "boolean_array_val = np.full((len(allsides_source_val), ), False)\n",
    "boolean_array_test = np.full((len(allsides_source_test), ), False)\n",
    "\n",
    "for source in unwanted_sources:\n",
    "    boolean_array_train += allsides_source_train==source\n",
    "    boolean_array_val += allsides_source_val==source \n",
    "    boolean_array_test += allsides_source_test==source \n",
    "# boolean to remove aggregators\n",
    "inverted_boolean_array_train = np.invert(boolean_array_train)\n",
    "inverted_boolean_array_val = np.invert(boolean_array_val)\n",
    "inverted_boolean_array_test = np.invert(boolean_array_test)\n",
    "\n",
    "# bias\n",
    "bias_train = bias_train[inverted_boolean_array_train]\n",
    "bias_val = bias_val[inverted_boolean_array_val]\n",
    "bias_test = bias_test[inverted_boolean_array_test]\n",
    "\n",
    "# text \n",
    "text_train = text_train[inverted_boolean_array_train]\n",
    "text_val = text_val[inverted_boolean_array_val]\n",
    "text_test = text_test[inverted_boolean_array_test]\n",
    "\n",
    "# sources\n",
    "allsides_source_train = allsides_source_train[inverted_boolean_array_train]\n",
    "allsides_source_val = allsides_source_val[inverted_boolean_array_val]\n",
    "allsides_source_test = allsides_source_test[inverted_boolean_array_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating training, validation, and test sets for pytorch models\n",
    "train_set = TensorDataset(text_train, bias_train)\n",
    "val_set = TensorDataset(text_val, bias_val)\n",
    "test_set = TensorDataset(text_test, bias_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention' from 'Attention is all you need'\n",
    "    \"\"\"  \n",
    "    hidden_size = query.size(-1)\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(hidden_size)\n",
    "    # if mask is not None:\n",
    "    #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attention_weights = F.softmax(attention_scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    return torch.matmul(attention_weights, value), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds block consisting of an LSTM and a fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout_prob, apply_attention=False, last_layer=False, bidirectional=True):\n",
    "        \"\"\"\n",
    "        hidden_size: Number of hidden neurons in LSTM and number of feature inputs (size of embedding/size of \n",
    "                     hidden layer output in stacked LSTM )\n",
    "        last_layer: Indicates whether to add \"boom\" layer (False) or not (True)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.directions = 1 + bidirectional\n",
    "        self.last_layer = last_layer\n",
    "        self.apply_attention = apply_attention\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        self.layer_norm_input = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm_lstm = nn.LayerNorm(self.directions*hidden_size)\n",
    "        \n",
    "        if apply_attention:\n",
    "            self.layer_norm_querry = nn.LayerNorm(self.directions*hidden_size, eps=1e-12)\n",
    "            self.layer_norm_key = nn.LayerNorm(self.directions*hidden_size)\n",
    "            self.layer_norm_value = nn.LayerNorm(self.directions*hidden_size)\n",
    "\n",
    "            self.qs = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "            self.ks = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "            self.vs = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "\n",
    "            self.querry_projection = nn.Linear(self.directions*hidden_size,self.directions*hidden_size)\n",
    "            self.linear_over_param = nn.Linear(self.directions*hidden_size, 2*self.directions*hidden_size) \n",
    "\n",
    "        if not last_layer: \n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.boom_a = nn.Linear(hidden_size*self.directions,4*self.directions*hidden_size) \n",
    "            self.activation = nn.GELU() \n",
    "            self.boom_b = nn.Linear(4*self.directions*hidden_size, hidden_size) \n",
    "    \n",
    "    def forward(self, feature_input):\n",
    "        ### Adjust batch size in case of last batch being shorter \n",
    "        current_batch_size = feature_input.shape[0]\n",
    "        # Initilize hidden and cell state\n",
    "        hidden_0 = torch.zeros(self.directions, current_batch_size, self.hidden_size).to(device)\n",
    "        # Applying layer normalization to input\n",
    "        feature_input = self.layer_norm_input(feature_input)\n",
    "        # LSTM layer with embeddings/hidden-state inputs \n",
    "        lstm_out, (last_hidden,last_cell) = self.lstm(feature_input, (hidden_0,hidden_0))\n",
    "\n",
    "        if self.apply_attention:\n",
    "            # Taken from Merity (2019):\n",
    "            # matrix multiplication and layer normalization on querry\n",
    "            querry = self.layer_norm_querry(self.querry_projection(lstm_out))\n",
    "            # only layer normalization on key and value\n",
    "            key = self.layer_norm_key(lstm_out)\n",
    "            value = self.layer_norm_value(lstm_out)\n",
    "            # activation of parameter vectors\n",
    "            qs, ks, vs = torch.sigmoid(self.qs), torch.sigmoid(self.ks), torch.sigmoid(self.vs) \n",
    "            # over parameterizing of value parameter vector, using forget gate and candidate (Merity 2019, 6.4)\n",
    "            candidate, forget = self.linear_over_param(vs).split(self.directions*self.hidden_size, dim=-1) \n",
    "            vs = torch.sigmoid(forget) * torch.tanh(candidate) \n",
    "            # multiplaying parameter vectors with querry, key, and value respectively\n",
    "            q, k, v, = qs*querry, ks*key, vs*value \n",
    "            # apply scaled dot product attention\n",
    "            lstm_out, attention_weights = attention(q,k,v, dropout=self.dropout)\n",
    "\n",
    "        # Applying layer normalization to lstm output\n",
    "        lstm_out = self.layer_norm_lstm(lstm_out)\n",
    "\n",
    "        if self.last_layer:\n",
    "            return lstm_out, last_hidden\n",
    "        else:\n",
    "            # big fully connected layer taking shape(batch, seq_len, num_directions * hidden_size) \n",
    "            # and returning shape(batch, seq_len, hidden_size)\n",
    "            boom_out = self.boom_b(self.dropout(self.activation(self.boom_a(lstm_out))))\n",
    "            return boom_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, seq_length, hidden_size, num_labels, num_layers, \n",
    "                 vocabulary_size, dropout_prob = 0.1, bidirectional = True, attention_layer=False):\n",
    "        \"\"\"\n",
    "        seq_length: Length of input sequence (Text) NOT USED\n",
    "        hidden_size (==embedding_size): Number or hidden neurons in LSTM, also used for embedding size\n",
    "        num_labels: Number of target labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_layers = num_layers\n",
    "        self.directions = 1 + bidirectional\n",
    "        self.attention_layer = attention_layer\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary_size, hidden_size, padding_idx=0)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # last layer\n",
    "            if i==num_layers-1:\n",
    "                self.blocks.append(LSTMBlock(hidden_size, dropout_prob=0, last_layer=True)) \n",
    "            # second last layer with attention\n",
    "            elif i==num_layers-2:\n",
    "                self.blocks.append(LSTMBlock(hidden_size, dropout_prob=dropout_prob, apply_attention=self.attention_layer))\n",
    "            # other layers\n",
    "            else:\n",
    "                self.blocks.append(LSTMBlock(hidden_size, dropout_prob=dropout_prob)) \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier_a = nn.Linear(hidden_size*self.directions,4*self.directions*hidden_size) \n",
    "        self.activation = nn.GELU() # nn.Tanh()\n",
    "        self.classifier_b = nn.Linear(4*self.directions*hidden_size, hidden_size) \n",
    "        self.classifier_c = nn.Linear(hidden_size, num_labels) \n",
    "        \n",
    "    def forward(self, text):\n",
    "        ### Embeddings\n",
    "        embeddings = self.embedding(text)\n",
    "\n",
    "        ### LSTM + \"Boom\"-layer blocks\n",
    "        for i,block in enumerate(self.blocks):\n",
    "            # only single layer\n",
    "            if len(self.blocks)==1:\n",
    "                last_hidden = block(embeddings)\n",
    "            # first layer\n",
    "            elif i==0:\n",
    "                block_out = block(embeddings)\n",
    "            # last layer\n",
    "            elif i==len(self.blocks)-1:\n",
    "                lstm_out, last_hidden = block(block_out)\n",
    "            # other layers\n",
    "            else:\n",
    "                block_out = block(block_out)\n",
    "\n",
    "        if self.directions==2:\n",
    "            # adjust last hidden state output to shape (batch_size,directions*hidden_size), i.e. concatinating both directions\n",
    "            last_hidden = torch.cat((last_hidden[0,:,:],last_hidden[1,:,:]), axis=1) \n",
    "        \n",
    "        ### Classifier layer\n",
    "        output = self.classifier_b(self.dropout(self.activation(self.classifier_a(last_hidden))))\n",
    "        output = self.classifier_c(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fct(train_set, batch_size, optimizer, return_mse=True, batch_feedback=500, first_check=10, mixed_precision=False, \n",
    "              save_memory_usage=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Setting model to train mode (so dropout is applied)\n",
    "    model.train()\n",
    "    # creating iterable dataset devided into batches and shuffled\n",
    "    data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # tracking batches, loss, accuracy\n",
    "    total_batch_count = int(len(train_set)/batch_size)\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correctly_specified = 0\n",
    "    train_predicted_values = []\n",
    "    train_true_values = []\n",
    "    \n",
    "    # Tracking memory usage\n",
    "    if save_memory_usage:\n",
    "        ! nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -f memory_usage.csv # change csv-file name for memory usage here and at the end if wanted\n",
    "\n",
    "    # looping over batches\n",
    "    for text, label in data:\n",
    "        # sending tensors to GPU\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        # clearing gradients\n",
    "        optimizer.zero_grad()\n",
    "        # run through model\n",
    "        output = model(text)\n",
    "        # calculating loss    \n",
    "        loss = loss_fct(output, label)\n",
    "        # backpropagation\n",
    "        if mixed_precision:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # updating weights\n",
    "        optimizer.step()\n",
    " \n",
    "        # loss and metrices messures\n",
    "        train_loss += loss.item()\n",
    "        train_correctly_specified += (output.argmax(1) == label).sum().item()\n",
    "        \n",
    "        train_predicted_values.append(output.argmax(1))\n",
    "        train_true_values.append(label)\n",
    "        \n",
    "        # adding to batchcounter\n",
    "        batch_counter += 1\n",
    "        \n",
    "        if (batch_counter % batch_feedback == 0) or (batch_counter == first_check):\n",
    "            time_so_far = time.time() - start_time\n",
    "            minutes = int(time_so_far // 60)\n",
    "            seconds = int(time_so_far % 60)\n",
    "            average_progress_loss = train_loss/batch_counter\n",
    "            progress_acc = train_correctly_specified/(batch_counter*batch_size)  \n",
    "            print('-------------------------------------------------')\n",
    "            print(f'{batch_counter:5} of {total_batch_count:5} batches done after {minutes:3} min {seconds:2} sec')\n",
    "            print('-------------------------------------------------')\n",
    "            print(f'loss: {average_progress_loss:6.4}   |   acc: {progress_acc:6.4}')\n",
    "            print('-------------------------------------------------')\n",
    "            #adding memory value\n",
    "            if save_memory_usage:\n",
    "                ! nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits >> memory_usage.csv\n",
    "\n",
    "    \n",
    "    # loss\n",
    "    average_total_loss = train_loss/(len(train_set)/batch_size)\n",
    "    # accuracy\n",
    "    total_accuracy = train_correctly_specified/len(train_set) \n",
    "    # Predicted and true values\n",
    "    train_predicted_values = torch.cat(train_predicted_values).cpu().numpy()\n",
    "    train_true_values = torch.cat(train_true_values).cpu().numpy()\n",
    "    # Precision\n",
    "    train_precision = precision_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    train_recall = recall_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    train_f1_score = f1_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # Mean Squared Error\n",
    "    if return_mse:\n",
    "        train_mse = mean_squared_error(train_true_values,train_predicted_values)\n",
    "    else: \n",
    "        train_mse = None\n",
    "    \n",
    "    # Loading memory usage to get maxium\n",
    "    if save_memory_usage:\n",
    "        memory_usage = np.loadtxt('memory_usage.csv', dtype='int', delimiter = ',') # csv-file name\n",
    "        max_memory_usage = int(np.max(memory_usage))\n",
    "    else:\n",
    "        max_memory_usage = None\n",
    "\n",
    "    return average_total_loss, total_accuracy, train_precision, train_recall, train_f1_score, train_mse, max_memory_usage\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "    return sum(layer.numel() for layer in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation/Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function for validation after 1 epoch of training\n",
    "def val_fct(val_set, batch_size, return_mse=True):\n",
    "    print('----------- Validation/Test Start -----------')\n",
    "    # Setting model to evaluation mode (dropout is not applied)\n",
    "    model.eval()\n",
    "    # creating iterable dataset devided into batches, not shuffeled\n",
    "    data = DataLoader(val_set, batch_size = batch_size)\n",
    "    # setting up loss and accuracy variables\n",
    "    val_loss = 0\n",
    "    #val_correctly_specified = 0\n",
    "    val_predicted_values = []\n",
    "    val_true_values = []\n",
    "    # looping over batches\n",
    "    for text, label in data:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        # no gradient calculation during validation\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = loss_fct(output, label)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            #val_correctly_specified += (output.argmax(1) == label).sum().item()\n",
    "            val_predicted_values.append(output.argmax(1))\n",
    "            val_true_values.append(label)\n",
    "    \n",
    "    # loss\n",
    "    average_val_loss = val_loss/(len(val_set)/batch_size)\n",
    "    # true and predicted values\n",
    "    val_predicted_values = torch.cat(val_predicted_values).cpu().numpy()\n",
    "    val_true_values = torch.cat(val_true_values).cpu().numpy()\n",
    "    # Accuracy\n",
    "    val_accuracy = (val_predicted_values==val_true_values).sum().item()/len(val_set) #val_correctly_specified/len(val_set)\n",
    "    # Precision\n",
    "    val_precision = precision_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    val_recall = recall_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    val_f1_score = f1_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Mean squared error\n",
    "    if return_mse:\n",
    "        val_mse = mean_squared_error(val_true_values,val_predicted_values)\n",
    "    else:\n",
    "        val_mse = None\n",
    "        \n",
    "    return average_val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Device to run model on, either GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Model inputs\n",
    "seq_length = 500 #text_train.shape[1] # (test:100)\n",
    "hidden_size = 512\n",
    "num_labels = 5\n",
    "num_layers = 4\n",
    "vocabulary_size = 30522\n",
    "dropout_prob = 0.1\n",
    "bidirectional = True\n",
    "attention_layer = True\n",
    "### Hyperparameters\n",
    "batch_size = 16 ######################## 64\n",
    "# learning_rate = 2e-5\n",
    "learning_rates_list = [2e-5, 2e-5, 2e-5, 1e-5, 1e-5, 1e-5]\n",
    "### Use of nvidia apex for mixed precession calculations\n",
    "mixed_precision = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Initilize model\n",
    "model = Model(seq_length, hidden_size, num_labels, num_layers, vocabulary_size, \n",
    "              dropout_prob, bidirectional, attention_layer).to(device)\n",
    "\n",
    "### Loss function\n",
    "loss_fct = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 60,794,373\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters: {num_parameters(model):,}' )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_fct(deviation_case, num_epochs, seed):\n",
    "    '''\n",
    "    Function to train model for a given number of epochs and saving all necessery figures and model weights\n",
    "    '''\n",
    "    global model\n",
    "    ### Dictionary to save metrices\n",
    "    metric_scores = {'epoch': [], 'time': [], \n",
    "                     'train_loss': [], 'train_acc': [], 'train_precision': [], 'train_recall': [], 'train_f1_score': [], 'train_mse': [],\n",
    "                     'val_loss': [], 'val_acc': [], 'val_precision': [], 'val_recall': [], 'val_f1_score': [], 'val_mse': [],\n",
    "                     'test_loss': [], 'test_acc': [], 'test_precision': [], 'test_recall': [], 'test_f1_score': [], 'test_mse': [], 'memory': []}\n",
    "\n",
    "    print(f'--- Number of parameters: {num_parameters(model):,} ---') \n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        # choose learning rate for this epoch\n",
    "        learning_rate = learning_rates_list[epoch-1]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "        if mixed_precision:\n",
    "            model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") \n",
    "        print(f'+ Learning rate used in epoch {epoch}: {learning_rate} +')\n",
    "\n",
    "        # Training for 1 epoch\n",
    "        train_loss, train_acc, train_precision, train_recall, \\\n",
    "        train_f1_score, train_mse, max_memory_usage = train_fct(train_set, \n",
    "                                                                batch_size,\n",
    "                                                                optimizer,\n",
    "                                                                batch_feedback=500, \n",
    "                                                                first_check=100, \n",
    "                                                                mixed_precision = mixed_precision,\n",
    "                                                                save_memory_usage = True)     \n",
    "        # Validation\n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1_score, val_mse = val_fct(val_set, batch_size)\n",
    "        \n",
    "        # Testing\n",
    "        test_loss, test_acc, test_precision, test_recall, test_f1_score, test_mse = val_fct(test_set, batch_size)\n",
    "        \n",
    "        # Display progress\n",
    "        end_time = time.time() - epoch_start_time\n",
    "        minutes = int(end_time // 60)\n",
    "        seconds = int(end_time % 60)\n",
    "        print('+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +')\n",
    "        print(f'+ Epoch: {epoch} took {minutes:3} min, {seconds:2} sec                             +')\n",
    "        try:\n",
    "            print(f'+ Maximum memory usage: {max_memory_usage:5} MiB                           +')\n",
    "        except TypeError:\n",
    "            pass\n",
    "        print(f'+ (Training)   Loss: {train_loss:6.4}  |  Acc: {train_acc:6.4}  |  F1: {train_f1_score:6.4}  +')\n",
    "        print(f'+ (Validation) Loss: {val_loss:6.4}  |  Acc: {val_acc:6.4}  |  F1: {val_f1_score:6.4}  +')\n",
    "        print('+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +')\n",
    "\n",
    "        # saving metrices\n",
    "        current_epoch_score_metrics = ['epoch', 'time', \n",
    "                                       'train_loss', 'train_acc', 'train_precision', 'train_recall', 'train_f1_score', 'train_mse',\n",
    "                                       'val_loss', 'val_acc', 'val_precision', 'val_recall', 'val_f1_score', 'val_mse',\n",
    "                                       'test_loss', 'test_acc', 'test_precision', 'test_recall', 'test_f1_score', 'test_mse', 'memory']\n",
    "        current_epoch_score_values = [epoch, round(end_time/60,2), \n",
    "                                      train_loss, train_acc, train_precision, train_recall, train_f1_score, train_mse,\n",
    "                                      val_loss, val_acc, val_precision, val_recall, val_f1_score, val_mse,\n",
    "                                      test_loss, test_acc, test_precision, test_recall, test_f1_score, test_mse, \n",
    "                                      max_memory_usage]\n",
    "        for metric,value in zip(current_epoch_score_metrics, current_epoch_score_values):\n",
    "            metric_scores[metric].append(value)\n",
    "\n",
    "        # saving model weights \n",
    "        if mixed_precision:\n",
    "            checkpoint = {'model': model.state_dict(),\n",
    "                          'optimizer': optimizer.state_dict(),\n",
    "                          'amp': amp.state_dict()}\n",
    "\n",
    "            torch.save(checkpoint, f'dl_benchmark_weights/amp_checkpoint_{deviation_case}_epoch{epoch}.pt')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f'dl_benchmark_weights/model_weights_{deviation_case}_epoch{epoch}.pt')\n",
    "\n",
    "        # saving final scores\n",
    "        if epoch==num_epochs:\n",
    "            results = pd.DataFrame(metric_scores)\n",
    "            results.to_csv(f'dl_benchmark_scores/metric_scores_{deviation_case}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Number of parameters: 60,794,373 ---\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "+ Learning rate used in epoch 1: 2e-05 +\n",
      "-------------------------------------------------\n",
      "  100 of 10181 batches done after   0 min 38 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.542   |   acc: 0.3025\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "  500 of 10181 batches done after   3 min  2 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.352   |   acc:  0.417\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1000 of 10181 batches done after   6 min  3 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.241   |   acc: 0.4747\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1500 of 10181 batches done after   9 min  6 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.171   |   acc: 0.5098\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2000 of 10181 batches done after  12 min  8 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.126   |   acc: 0.5322\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2500 of 10181 batches done after  15 min 11 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.089   |   acc: 0.5492\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3000 of 10181 batches done after  18 min 14 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.055   |   acc: 0.5639\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "-------------------------------------------------\n",
      " 3500 of 10181 batches done after  21 min 17 sec\n",
      "-------------------------------------------------\n",
      "loss:   1.03   |   acc:  0.575\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4000 of 10181 batches done after  24 min 19 sec\n",
      "-------------------------------------------------\n",
      "loss:  1.005   |   acc: 0.5871\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4500 of 10181 batches done after  27 min 22 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.9821   |   acc: 0.5978\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5000 of 10181 batches done after  30 min 25 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.9612   |   acc: 0.6075\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "-------------------------------------------------\n",
      " 5500 of 10181 batches done after  33 min 28 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.943   |   acc: 0.6163\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6000 of 10181 batches done after  36 min 31 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.9259   |   acc: 0.6245\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6500 of 10181 batches done after  39 min 34 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.911   |   acc: 0.6311\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 7000 of 10181 batches done after  42 min 37 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8974   |   acc: 0.6371\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
      "-------------------------------------------------\n",
      " 7500 of 10181 batches done after  45 min 39 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8854   |   acc: 0.6424\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 8000 of 10181 batches done after  48 min 42 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8721   |   acc: 0.6482\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8500 of 10181 batches done after  51 min 45 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8604   |   acc: 0.6532\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9000 of 10181 batches done after  54 min 48 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8495   |   acc: 0.6581\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9500 of 10181 batches done after  57 min 51 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8386   |   acc:  0.663\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "10000 of 10181 batches done after  60 min 53 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.8289   |   acc: 0.6673\n",
      "-------------------------------------------------\n",
      "----------- Validation/Test Start -----------\n",
      "----------- Validation/Test Start -----------\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "+ Epoch: 1 took  67 min, 34 sec                             +\n",
      "+ Maximum memory usage:  3810 MiB                           +\n",
      "+ (Training)   Loss: 0.8256  |  Acc: 0.6688  |  F1: 0.6707  +\n",
      "+ (Validation) Loss: 0.6375  |  Acc: 0.7502  |  F1: 0.7503  +\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "+ Learning rate used in epoch 2: 2e-05 +\n",
      "-------------------------------------------------\n",
      "  100 of 10181 batches done after   0 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.638   |   acc: 0.7469\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "  500 of 10181 batches done after   3 min  2 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.6086   |   acc: 0.7686\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1000 of 10181 batches done after   6 min  4 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5968   |   acc: 0.7682\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1500 of 10181 batches done after   9 min  7 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5913   |   acc: 0.7713\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 2000 of 10181 batches done after  12 min 10 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5894   |   acc: 0.7723\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      " 2500 of 10181 batches done after  15 min 13 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5866   |   acc: 0.7744\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3000 of 10181 batches done after  18 min 16 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5821   |   acc: 0.7759\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3500 of 10181 batches done after  21 min 19 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5806   |   acc: 0.7766\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4000 of 10181 batches done after  24 min 22 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5783   |   acc:  0.778\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 4500 of 10181 batches done after  27 min 24 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5751   |   acc:  0.779\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5000 of 10181 batches done after  30 min 27 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5733   |   acc: 0.7798\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5500 of 10181 batches done after  33 min 30 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5709   |   acc: 0.7806\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6000 of 10181 batches done after  36 min 33 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.568   |   acc: 0.7816\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6500 of 10181 batches done after  39 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5663   |   acc: 0.7822\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 7000 of 10181 batches done after  42 min 39 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5644   |   acc: 0.7831\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 7500 of 10181 batches done after  45 min 42 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5628   |   acc: 0.7836\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8000 of 10181 batches done after  48 min 44 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5604   |   acc: 0.7847\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8500 of 10181 batches done after  51 min 47 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5579   |   acc: 0.7856\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 9000 of 10181 batches done after  54 min 50 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5564   |   acc: 0.7863\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9500 of 10181 batches done after  57 min 53 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5539   |   acc: 0.7874\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "10000 of 10181 batches done after  60 min 55 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.5516   |   acc: 0.7884\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "----------- Validation/Test Start -----------\n",
      "----------- Validation/Test Start -----------\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "+ Epoch: 2 took  67 min, 40 sec                             +\n",
      "+ Maximum memory usage:  4460 MiB                           +\n",
      "+ (Training)   Loss: 0.5509  |  Acc: 0.7888  |  F1: 0.7896  +\n",
      "+ (Validation) Loss: 0.5049  |  Acc: 0.8078  |  F1:  0.808  +\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "+ Learning rate used in epoch 3: 2e-05 +\n",
      "-------------------------------------------------\n",
      "  100 of 10181 batches done after   0 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.461   |   acc: 0.8263\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "  500 of 10181 batches done after   3 min  2 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4897   |   acc: 0.8147\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 1000 of 10181 batches done after   6 min  4 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4763   |   acc: 0.8204\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1500 of 10181 batches done after   9 min  7 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4684   |   acc: 0.8248\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2000 of 10181 batches done after  12 min  9 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4632   |   acc: 0.8267\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2500 of 10181 batches done after  15 min 12 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4613   |   acc: 0.8284\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 3000 of 10181 batches done after  18 min 15 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4602   |   acc: 0.8286\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3500 of 10181 batches done after  21 min 18 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4587   |   acc: 0.8289\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4000 of 10181 batches done after  24 min 20 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4591   |   acc: 0.8282\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4500 of 10181 batches done after  27 min 23 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4581   |   acc: 0.8279\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5000 of 10181 batches done after  30 min 26 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4557   |   acc: 0.8295\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 5500 of 10181 batches done after  33 min 28 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4537   |   acc: 0.8299\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6000 of 10181 batches done after  36 min 31 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4534   |   acc: 0.8296\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      " 6500 of 10181 batches done after  39 min 34 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.451   |   acc: 0.8308\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 7000 of 10181 batches done after  42 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4503   |   acc:  0.831\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 7500 of 10181 batches done after  45 min 39 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4504   |   acc:  0.831\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8000 of 10181 batches done after  48 min 42 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4488   |   acc: 0.8317\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8500 of 10181 batches done after  51 min 44 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4472   |   acc: 0.8323\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9000 of 10181 batches done after  54 min 47 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4466   |   acc: 0.8324\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 9500 of 10181 batches done after  57 min 50 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4448   |   acc: 0.8331\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "10000 of 10181 batches done after  60 min 52 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.4434   |   acc: 0.8337\n",
      "-------------------------------------------------\n",
      "----------- Validation/Test Start -----------\n",
      "----------- Validation/Test Start -----------\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "+ Epoch: 3 took  67 min, 35 sec                             +\n",
      "+ Maximum memory usage:  4460 MiB                           +\n",
      "+ (Training)   Loss: 0.4432  |  Acc: 0.8338  |  F1: 0.8342  +\n",
      "+ (Validation) Loss:  0.454  |  Acc: 0.8305  |  F1: 0.8312  +\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "+ Learning rate used in epoch 4: 1e-05 +\n",
      "-------------------------------------------------\n",
      "  100 of 10181 batches done after   0 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3047   |   acc: 0.8869\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      "  500 of 10181 batches done after   3 min  2 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3176   |   acc: 0.8826\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1000 of 10181 batches done after   6 min  4 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3259   |   acc: 0.8798\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1500 of 10181 batches done after   9 min  7 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3241   |   acc: 0.8811\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2000 of 10181 batches done after  12 min 10 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3244   |   acc:  0.881\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 2500 of 10181 batches done after  15 min 13 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3249   |   acc: 0.8802\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3000 of 10181 batches done after  18 min 15 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3251   |   acc: 0.8795\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3500 of 10181 batches done after  21 min 18 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3221   |   acc: 0.8809\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4000 of 10181 batches done after  24 min 21 sec\n",
      "-------------------------------------------------\n",
      "loss:   0.32   |   acc: 0.8817\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 4500 of 10181 batches done after  27 min 24 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3212   |   acc:  0.881\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5000 of 10181 batches done after  30 min 26 sec\n",
      "-------------------------------------------------\n",
      "loss:  0.321   |   acc: 0.8809\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5500 of 10181 batches done after  33 min 29 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3209   |   acc: 0.8807\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6000 of 10181 batches done after  36 min 31 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3205   |   acc: 0.8811\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 6500 of 10181 batches done after  39 min 33 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3199   |   acc: 0.8813\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 7000 of 10181 batches done after  42 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3194   |   acc: 0.8813\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 7500 of 10181 batches done after  45 min 38 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3192   |   acc: 0.8815\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8000 of 10181 batches done after  48 min 40 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3197   |   acc: 0.8813\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8500 of 10181 batches done after  51 min 42 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3194   |   acc: 0.8816\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 9000 of 10181 batches done after  54 min 44 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3201   |   acc: 0.8814\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9500 of 10181 batches done after  57 min 46 sec\n",
      "-------------------------------------------------\n",
      "loss:   0.32   |   acc: 0.8814\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "10000 of 10181 batches done after  60 min 47 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.3192   |   acc: 0.8817\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Validation/Test Start -----------\n",
      "----------- Validation/Test Start -----------\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "+ Epoch: 4 took  67 min, 28 sec                             +\n",
      "+ Maximum memory usage:  4586 MiB                           +\n",
      "+ (Training)   Loss: 0.3188  |  Acc: 0.8819  |  F1:  0.882  +\n",
      "+ (Validation) Loss: 0.4302  |  Acc: 0.8466  |  F1: 0.8467  +\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "+ Learning rate used in epoch 5: 1e-05 +\n",
      "-------------------------------------------------\n",
      "  100 of 10181 batches done after   0 min 36 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2353   |   acc: 0.9163\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      "  500 of 10181 batches done after   3 min  1 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2471   |   acc: 0.9123\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 1000 of 10181 batches done after   6 min  2 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2451   |   acc: 0.9125\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "-------------------------------------------------\n",
      " 1500 of 10181 batches done after   9 min  3 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2458   |   acc: 0.9113\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2000 of 10181 batches done after  12 min  4 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2455   |   acc:  0.911\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 2500 of 10181 batches done after  15 min  6 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2484   |   acc: 0.9098\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3000 of 10181 batches done after  18 min  7 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2496   |   acc: 0.9095\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 3500 of 10181 batches done after  21 min  9 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2506   |   acc: 0.9091\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4000 of 10181 batches done after  24 min 11 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2507   |   acc:  0.909\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 4500 of 10181 batches done after  27 min 13 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2512   |   acc: 0.9089\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 5000 of 10181 batches done after  30 min 15 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2509   |   acc: 0.9088\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "-------------------------------------------------\n",
      " 5500 of 10181 batches done after  33 min 17 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2517   |   acc: 0.9084\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6000 of 10181 batches done after  36 min 18 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2516   |   acc: 0.9086\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 6500 of 10181 batches done after  39 min 20 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2509   |   acc: 0.9088\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 7000 of 10181 batches done after  42 min 22 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2515   |   acc: 0.9088\n",
      "-------------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "-------------------------------------------------\n",
      " 7500 of 10181 batches done after  45 min 24 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2522   |   acc: 0.9084\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8000 of 10181 batches done after  48 min 25 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2523   |   acc: 0.9084\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 8500 of 10181 batches done after  51 min 27 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2525   |   acc: 0.9085\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9000 of 10181 batches done after  54 min 28 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2528   |   acc: 0.9084\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      " 9500 of 10181 batches done after  57 min 30 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2532   |   acc: 0.9083\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "10000 of 10181 batches done after  60 min 32 sec\n",
      "-------------------------------------------------\n",
      "loss: 0.2536   |   acc: 0.9082\n",
      "-------------------------------------------------\n",
      "----------- Validation/Test Start -----------\n",
      "----------- Validation/Test Start -----------\n"
     ]
    }
   ],
   "source": [
    "### #RUN ###\n",
    "run = 1  ###\n",
    "############\n",
    "num_epochs = 5\n",
    "deviation_case = f'dl_benchmark_allsides_batch_16_all_removed_rerun_{run}'\n",
    "seed = 19 + run #20,21,22\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model_training_fct(deviation_case, num_epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping instance\n",
    "! gcloud compute instances stop t4-instance --zone=europe-west4-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
