{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting GPU type\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    print('Is available')\n",
    "else:\n",
    "    print('is not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to media-bias-prediction repository \n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "# set working directory to deep learning models directory\n",
    "os.chdir(os.path.join(repo_path, 'deep_learning_models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading tensors \n",
    "os.chdir(os.path.join(repo_path, 'data_preparation','allsides_data'))\n",
    "\n",
    "bias_train = torch.load('allsides_bias_train.pt')\n",
    "bias_val = torch.load('allsides_bias_val.pt')\n",
    "bias_test = torch.load('allsides_bias_test.pt')\n",
    "\n",
    "\n",
    "text_train = torch.load('allsides_duplicates_removed_contents_text_train.pt')\n",
    "text_val = torch.load('allsides_contents_text_val.pt')\n",
    "text_test = torch.load('allsides_contents_text_test.pt')\n",
    "\n",
    "os.chdir(os.path.join(repo_path, 'deep_learning_models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing news aggregators, tabloids, and wrongly labeled source from tensors\n",
    "os.chdir(os.path.join(repo_path, 'data_preparation','allsides_data'))\n",
    "\n",
    "allsides_source_train = np.load('allsides_source_train.npy', allow_pickle=True).flatten()\n",
    "allsides_source_val = np.load('allsides_source_val.npy', allow_pickle=True).flatten()\n",
    "allsides_source_test = np.load('allsides_source_test.npy', allow_pickle=True).flatten()\n",
    "\n",
    "# sources to be removed:\n",
    "wrongly_labeled = ['RightWingWatch']\n",
    "news_aggregators = ['Drudge Report', 'Real Clear Politics', 'Yahoo News', 'RightWingWatch'] \n",
    "tabloids = ['New York Daily News', 'Daily Mail', 'New York Post'] \n",
    "unwanted_sources = wrongly_labeled + news_aggregators + tabloids\n",
    "\n",
    "# creating boolean array to mark unwanted sources\n",
    "boolean_array_train = np.full((len(allsides_source_train), ), False)\n",
    "boolean_array_val = np.full((len(allsides_source_val), ), False)\n",
    "boolean_array_test = np.full((len(allsides_source_test), ), False)\n",
    "\n",
    "for source in unwanted_sources:\n",
    "    boolean_array_train += allsides_source_train==source\n",
    "    boolean_array_val += allsides_source_val==source \n",
    "    boolean_array_test += allsides_source_test==source \n",
    "    \n",
    "# boolean to remove aggregators\n",
    "inverted_boolean_array_train = np.invert(boolean_array_train)\n",
    "inverted_boolean_array_val = np.invert(boolean_array_val)\n",
    "inverted_boolean_array_test = np.invert(boolean_array_test)\n",
    "\n",
    "# bias\n",
    "bias_train = bias_train[inverted_boolean_array_train]\n",
    "bias_val = bias_val[inverted_boolean_array_val]\n",
    "bias_test = bias_test[inverted_boolean_array_test]\n",
    "\n",
    "# text \n",
    "text_train = text_train[inverted_boolean_array_train]\n",
    "text_val = text_val[inverted_boolean_array_val]\n",
    "text_test = text_test[inverted_boolean_array_test]\n",
    "\n",
    "# sources\n",
    "allsides_source_train = allsides_source_train[inverted_boolean_array_train]\n",
    "allsides_source_val = allsides_source_val[inverted_boolean_array_val]\n",
    "allsides_source_test = allsides_source_test[inverted_boolean_array_test]\n",
    "\n",
    "os.chdir(os.path.join(repo_path, 'deep_learning_models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating training, validation, and test sets for pytorch model\n",
    "train_set = TensorDataset(text_train, bias_train)\n",
    "val_set = TensorDataset(text_val, bias_val)\n",
    "test_set = TensorDataset(text_test, bias_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention' from 'Attention is all you need'\n",
    "    \"\"\"  \n",
    "    hidden_size = query.size(-1)\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(hidden_size)\n",
    "    attention_weights = F.softmax(attention_scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    return torch.matmul(attention_weights, value), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds block consisting of an LSTM and a fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout_prob, apply_attention=False, last_layer=False, bidirectional=True):\n",
    "        \"\"\"\n",
    "        hidden_size: Number of hidden neurons in LSTM and number of feature inputs (size of embedding/size of \n",
    "                     hidden layer output in stacked LSTM )\n",
    "        last_layer: Indicates whether to add \"boom\" layer (False) or not (True)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.directions = 1 + bidirectional\n",
    "        self.last_layer = last_layer\n",
    "        self.apply_attention = apply_attention\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        self.layer_norm_input = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm_lstm = nn.LayerNorm(self.directions*hidden_size)\n",
    "        \n",
    "        if apply_attention:\n",
    "            self.layer_norm_querry = nn.LayerNorm(self.directions*hidden_size, eps=1e-12)\n",
    "            self.layer_norm_key = nn.LayerNorm(self.directions*hidden_size)\n",
    "            self.layer_norm_value = nn.LayerNorm(self.directions*hidden_size)\n",
    "\n",
    "            self.qs = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "            self.ks = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "            self.vs = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "\n",
    "            self.querry_projection = nn.Linear(self.directions*hidden_size,self.directions*hidden_size)\n",
    "            self.linear_over_param = nn.Linear(self.directions*hidden_size, 2*self.directions*hidden_size) \n",
    "\n",
    "        if not last_layer: \n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.boom_a = nn.Linear(hidden_size*self.directions,4*self.directions*hidden_size) \n",
    "            self.activation = nn.GELU() \n",
    "            self.boom_b = nn.Linear(4*self.directions*hidden_size, hidden_size) \n",
    "    \n",
    "    def forward(self, feature_input):\n",
    "        ### Adjust batch size in case of last batch being shorter \n",
    "        current_batch_size = feature_input.shape[0]\n",
    "        # Initilize hidden and cell state\n",
    "        hidden_0 = torch.zeros(self.directions, current_batch_size, self.hidden_size).to(device)\n",
    "        # Applying layer normalization to input\n",
    "        feature_input = self.layer_norm_input(feature_input)\n",
    "        # LSTM layer with embeddings/hidden-state inputs \n",
    "        lstm_out, (last_hidden,last_cell) = self.lstm(feature_input, (hidden_0,hidden_0))\n",
    "\n",
    "        if self.apply_attention:\n",
    "            # Taken from Merity (2019):\n",
    "            # matrix multiplication and layer normalization on querry\n",
    "            querry = self.layer_norm_querry(self.querry_projection(lstm_out))\n",
    "            # only layer normalization on key and value\n",
    "            key = self.layer_norm_key(lstm_out)\n",
    "            value = self.layer_norm_value(lstm_out)\n",
    "            # activation of parameter vectors\n",
    "            qs, ks, vs = torch.sigmoid(self.qs), torch.sigmoid(self.ks), torch.sigmoid(self.vs) \n",
    "            # over parameterizing of value parameter vector, using forget gate and candidate (Merity 2019, 6.4)\n",
    "            candidate, forget = self.linear_over_param(vs).split(self.directions*self.hidden_size, dim=-1) \n",
    "            vs = torch.sigmoid(forget) * torch.tanh(candidate) \n",
    "            # multiplaying parameter vectors with querry, key, and value respectively\n",
    "            q, k, v, = qs*querry, ks*key, vs*value \n",
    "            # apply scaled dot product attention\n",
    "            lstm_out, attention_weights = attention(q,k,v, dropout=self.dropout)\n",
    "\n",
    "        # Applying layer normalization to lstm output\n",
    "        lstm_out = self.layer_norm_lstm(lstm_out)\n",
    "\n",
    "        if self.last_layer:\n",
    "            return lstm_out, last_hidden\n",
    "        else:\n",
    "            # big fully connected layer taking shape(batch, seq_len, num_directions * hidden_size) \n",
    "            # and returning shape(batch, seq_len, hidden_size)\n",
    "            boom_out = self.boom_b(self.dropout(self.activation(self.boom_a(lstm_out))))\n",
    "            return boom_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, num_layers, \n",
    "                 vocabulary_size, dropout_prob = 0.1, bidirectional = True, attention_layer=False):\n",
    "        \"\"\"\n",
    "        hidden_size: Number or hidden neurons in LSTM, also used for embedding size\n",
    "        num_labels: Number of target labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_layers = num_layers\n",
    "        self.directions = 1 + bidirectional\n",
    "        self.attention_layer = attention_layer\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary_size, hidden_size, padding_idx=0)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # last layer\n",
    "            if i==num_layers-1:\n",
    "                self.blocks.append(LSTMBlock(hidden_size, dropout_prob=0, last_layer=True)) \n",
    "            # second last layer with attention\n",
    "            elif i==num_layers-2:\n",
    "                self.blocks.append(LSTMBlock(hidden_size, dropout_prob=dropout_prob, apply_attention=self.attention_layer))\n",
    "            # other layers\n",
    "            else:\n",
    "                self.blocks.append(LSTMBlock(hidden_size, dropout_prob=dropout_prob)) \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier_a = nn.Linear(hidden_size*self.directions,4*self.directions*hidden_size) \n",
    "        self.activation = nn.GELU() \n",
    "        self.classifier_b = nn.Linear(4*self.directions*hidden_size, hidden_size) \n",
    "        self.classifier_c = nn.Linear(hidden_size, num_labels) \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embeddings = self.embedding(text)\n",
    "\n",
    "        ### LSTM + \"Boom\"-layer blocks\n",
    "        for i,block in enumerate(self.blocks):\n",
    "            # only single layer\n",
    "            if len(self.blocks)==1:\n",
    "                last_hidden = block(embeddings)\n",
    "            # first layer\n",
    "            elif i==0:\n",
    "                block_out = block(embeddings)\n",
    "            # last layer\n",
    "            elif i==len(self.blocks)-1:\n",
    "                lstm_out, last_hidden = block(block_out)\n",
    "            # other layers\n",
    "            else:\n",
    "                block_out = block(block_out)\n",
    "\n",
    "        if self.directions==2:\n",
    "            # adjust last hidden state output to shape (batch_size,directions*hidden_size), i.e. concatinating both directions\n",
    "            last_hidden = torch.cat((last_hidden[0,:,:],last_hidden[1,:,:]), axis=1) \n",
    "        \n",
    "        ### Classifier layer\n",
    "        output = self.classifier_b(self.dropout(self.activation(self.classifier_a(last_hidden))))\n",
    "        output = self.classifier_c(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fct(train_set, batch_size, optimizer, return_mse=True, batch_feedback=500, first_check=10, mixed_precision=False, \n",
    "              save_memory_usage=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Setting model to train mode (so dropout is applied)\n",
    "    model.train()\n",
    "    # creating iterable dataset devided into batches and shuffled\n",
    "    data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # tracking batches, loss, accuracy\n",
    "    total_batch_count = int(len(train_set)/batch_size)\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correctly_specified = 0\n",
    "    train_predicted_values = []\n",
    "    train_true_values = []\n",
    "    \n",
    "    # Tracking memory usage\n",
    "    if save_memory_usage:\n",
    "        ! nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -f memory_usage.csv \n",
    "\n",
    "    # looping over batches\n",
    "    for text, label in data:\n",
    "        # sending tensors to GPU\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        # clearing gradients\n",
    "        optimizer.zero_grad()\n",
    "        # run through model\n",
    "        output = model(text)\n",
    "        # calculating loss    \n",
    "        loss = loss_fct(output, label)\n",
    "        # backpropagation\n",
    "        if mixed_precision:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # updating weights\n",
    "        optimizer.step()\n",
    " \n",
    "        # loss and metrices messures\n",
    "        train_loss += loss.item()\n",
    "        train_correctly_specified += (output.argmax(1) == label).sum().item()\n",
    "        \n",
    "        train_predicted_values.append(output.argmax(1))\n",
    "        train_true_values.append(label)\n",
    "        \n",
    "        batch_counter += 1\n",
    "        \n",
    "        if (batch_counter % batch_feedback == 0) or (batch_counter == first_check):\n",
    "            time_so_far = time.time() - start_time\n",
    "            minutes = int(time_so_far // 60)\n",
    "            seconds = int(time_so_far % 60)\n",
    "            average_progress_loss = train_loss/batch_counter\n",
    "            progress_acc = train_correctly_specified/(batch_counter*batch_size)  \n",
    "            print('-------------------------------------------------')\n",
    "            print(f'{batch_counter:5} of {total_batch_count:5} batches done after {minutes:3} min {seconds:2} sec')\n",
    "            print('-------------------------------------------------')\n",
    "            print(f'loss: {average_progress_loss:6.4}   |   acc: {progress_acc:6.4}')\n",
    "            print('-------------------------------------------------')\n",
    "            #adding memory value\n",
    "            if save_memory_usage:\n",
    "                ! nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits >> memory_usage.csv\n",
    "\n",
    "    \n",
    "    # loss\n",
    "    average_total_loss = train_loss/(len(train_set)/batch_size)\n",
    "    # accuracy\n",
    "    total_accuracy = train_correctly_specified/len(train_set) \n",
    "    # Predicted and true values\n",
    "    train_predicted_values = torch.cat(train_predicted_values).cpu().numpy()\n",
    "    train_true_values = torch.cat(train_true_values).cpu().numpy()\n",
    "    # Precision\n",
    "    train_precision = precision_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    train_recall = recall_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    train_f1_score = f1_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # Mean Squared Error\n",
    "    if return_mse:\n",
    "        train_mse = mean_squared_error(train_true_values,train_predicted_values)\n",
    "    else: \n",
    "        train_mse = None\n",
    "    \n",
    "    # Loading memory usage to get maxium\n",
    "    if save_memory_usage:\n",
    "        memory_usage = np.loadtxt('memory_usage.csv', dtype='int', delimiter = ',') # csv-file name\n",
    "        max_memory_usage = int(np.max(memory_usage))\n",
    "    else:\n",
    "        max_memory_usage = None\n",
    "\n",
    "    return average_total_loss, total_accuracy, train_precision, train_recall, train_f1_score, train_mse, max_memory_usage\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "    return sum(layer.numel() for layer in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation/Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function for validation after 1 epoch of training\n",
    "def val_fct(val_set, batch_size, return_mse=True):\n",
    "    print('----------- Validation/Test Start -----------')\n",
    "    # Setting model to evaluation mode (dropout is not applied)\n",
    "    model.eval()\n",
    "    # creating iterable dataset devided into batches, not shuffeled\n",
    "    data = DataLoader(val_set, batch_size = batch_size)\n",
    "    # setting up loss and accuracy variables\n",
    "    val_loss = 0\n",
    "    val_predicted_values = []\n",
    "    val_true_values = []\n",
    "    # looping over batches\n",
    "    for text, label in data:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        # no gradient calculation during validation\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = loss_fct(output, label)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_predicted_values.append(output.argmax(1))\n",
    "            val_true_values.append(label)\n",
    "    \n",
    "    # loss\n",
    "    average_val_loss = val_loss/(len(val_set)/batch_size)\n",
    "    # true and predicted values\n",
    "    val_predicted_values = torch.cat(val_predicted_values).cpu().numpy()\n",
    "    val_true_values = torch.cat(val_true_values).cpu().numpy()\n",
    "    # Accuracy\n",
    "    val_accuracy = (val_predicted_values==val_true_values).sum().item()/len(val_set) \n",
    "    # Precision\n",
    "    val_precision = precision_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    val_recall = recall_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    val_f1_score = f1_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Mean squared error\n",
    "    if return_mse:\n",
    "        val_mse = mean_squared_error(val_true_values,val_predicted_values)\n",
    "    else:\n",
    "        val_mse = None\n",
    "        \n",
    "    return average_val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Device to run model on, either GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Model inputs\n",
    "hidden_size = 512\n",
    "num_labels = 5\n",
    "num_layers = 4\n",
    "vocabulary_size = 30522\n",
    "dropout_prob = 0.1\n",
    "bidirectional = True\n",
    "attention_layer = True\n",
    "### Hyperparameters\n",
    "batch_size = 64 ######## alternatively to save memory: 16\n",
    "learning_rates_list = [2e-5, 2e-5, 2e-5, 1e-5, 1e-5, 1e-5]\n",
    "### Use of nvidia apex for mixed precession calculations\n",
    "mixed_precision = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Initilize model\n",
    "model = Model(hidden_size, num_labels, num_layers, vocabulary_size, \n",
    "              dropout_prob, bidirectional, attention_layer).to(device)\n",
    "\n",
    "### Loss function\n",
    "loss_fct = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_fct(deviation_case, num_epochs, seed):\n",
    "    '''\n",
    "    Function to train model for a given number of epochs and saving all necessery figures and model weights\n",
    "    '''\n",
    "    global model\n",
    "    ### Dictionary to save metrices\n",
    "    metric_scores = {'epoch': [], 'time': [], \n",
    "                     'train_loss': [], 'train_acc': [], 'train_precision': [], 'train_recall': [], 'train_f1_score': [], 'train_mse': [],\n",
    "                     'val_loss': [], 'val_acc': [], 'val_precision': [], 'val_recall': [], 'val_f1_score': [], 'val_mse': [],\n",
    "                     'test_loss': [], 'test_acc': [], 'test_precision': [], 'test_recall': [], 'test_f1_score': [], 'test_mse': [], 'memory': []}\n",
    "\n",
    "    print(f'--- Number of parameters: {num_parameters(model):,} ---') \n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        # choose learning rate for this epoch\n",
    "        learning_rate = learning_rates_list[epoch-1]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "        \n",
    "        if mixed_precision:\n",
    "            model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") \n",
    "        print(f'+ Learning rate used in epoch {epoch}: {learning_rate} +')\n",
    "\n",
    "        # Training for 1 epoch\n",
    "        train_loss, train_acc, train_precision, train_recall, \\\n",
    "        train_f1_score, train_mse, max_memory_usage = train_fct(train_set, \n",
    "                                                                batch_size,\n",
    "                                                                optimizer,\n",
    "                                                                batch_feedback=500, \n",
    "                                                                first_check=100, \n",
    "                                                                mixed_precision = mixed_precision,\n",
    "                                                                save_memory_usage = True)     \n",
    "        # Validation\n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1_score, val_mse = val_fct(val_set, batch_size)\n",
    "        \n",
    "        # Testing\n",
    "        test_loss, test_acc, test_precision, test_recall, test_f1_score, test_mse = val_fct(test_set, batch_size)\n",
    "        \n",
    "        # Display progress\n",
    "        end_time = time.time() - epoch_start_time\n",
    "        minutes = int(end_time // 60)\n",
    "        seconds = int(end_time % 60)\n",
    "        print('+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +')\n",
    "        print(f'+ Epoch: {epoch} took {minutes:3} min, {seconds:2} sec                             +')\n",
    "        try:\n",
    "            print(f'+ Maximum memory usage: {max_memory_usage:5} MiB                           +')\n",
    "        except TypeError:\n",
    "            pass\n",
    "        print(f'+ (Training)   Loss: {train_loss:6.4}  |  Acc: {train_acc:6.4}  |  F1: {train_f1_score:6.4}  +')\n",
    "        print(f'+ (Validation) Loss: {val_loss:6.4}  |  Acc: {val_acc:6.4}  |  F1: {val_f1_score:6.4}  +')\n",
    "        print('+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +')\n",
    "\n",
    "        # saving metrices\n",
    "        current_epoch_score_metrics = ['epoch', 'time', \n",
    "                                       'train_loss', 'train_acc', 'train_precision', 'train_recall', 'train_f1_score', 'train_mse',\n",
    "                                       'val_loss', 'val_acc', 'val_precision', 'val_recall', 'val_f1_score', 'val_mse',\n",
    "                                       'test_loss', 'test_acc', 'test_precision', 'test_recall', 'test_f1_score', 'test_mse', 'memory']\n",
    "        current_epoch_score_values = [epoch, round(end_time/60,2), \n",
    "                                      train_loss, train_acc, train_precision, train_recall, train_f1_score, train_mse,\n",
    "                                      val_loss, val_acc, val_precision, val_recall, val_f1_score, val_mse,\n",
    "                                      test_loss, test_acc, test_precision, test_recall, test_f1_score, test_mse, \n",
    "                                      max_memory_usage]\n",
    "        for metric,value in zip(current_epoch_score_metrics, current_epoch_score_values):\n",
    "            metric_scores[metric].append(value)\n",
    "\n",
    "        # saving model weights \n",
    "        if mixed_precision:\n",
    "            checkpoint = {'model': model.state_dict(),\n",
    "                          'optimizer': optimizer.state_dict(),\n",
    "                          'amp': amp.state_dict()}\n",
    "\n",
    "            torch.save(checkpoint, f'dl_benchmark_weights/amp_checkpoint_{deviation_case}_epoch{epoch}.pt')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f'dl_benchmark_weights/model_weights_{deviation_case}_epoch{epoch}.pt')\n",
    "\n",
    "        # saving final scores\n",
    "        if epoch==num_epochs:\n",
    "            results = pd.DataFrame(metric_scores)\n",
    "            results.to_csv(f'dl_benchmark_scores/metric_scores_{deviation_case}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #RUN ###\n",
    "run = 1  ###\n",
    "############\n",
    "num_epochs = 5\n",
    "### name experiment\n",
    "deviation_case = f'dl_benchmark_allsides_all_removed_rerun_{run}'\n",
    "seed = 19 + run #20,21,22\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model_training_fct(deviation_case, num_epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping instance\n",
    "! gcloud compute instances stop t4-instance --zone=europe-west4-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
