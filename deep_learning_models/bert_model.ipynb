{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation of BERT based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "Is available\n"
     ]
    }
   ],
   "source": [
    "### Getting GPU type\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    print('Is available')\n",
    "else:\n",
    "    print('GPU is not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to media-bias-prediction repository (if it breaks, set media-bias-prediction absolute path manually)\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "# set working directory to deep learning models directory\n",
    "os.chdir(os.path.join(repo_path, 'deep_learning_models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove news aggregatores from dataset\n",
    "remove_aggregators = True\n",
    "\n",
    "### Remove tabloid articles from dataset\n",
    "remove_tabloids = True\n",
    "\n",
    "### Use training data of which frequent sentences are removed\n",
    "remove_duplicates = True\n",
    "\n",
    "### Choose if and which one of two groups of sources should be removed from training data \n",
    "### (small and large with each containing one source per bias category)\n",
    "remove_source_group = None # 'small' # 'large' \n",
    "\n",
    "### Apply cost sensitive loss\n",
    "cost_sensitive = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment is run at the end of the notebook. There, number of run and name of the experiment need to be chosen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to data directory\n",
    "os.chdir(os.path.join(repo_path, 'data_preparation','allsides_data'))\n",
    "\n",
    "bias_train = torch.load('allsides_bias_train.pt')\n",
    "bias_val = torch.load('allsides_bias_val.pt')\n",
    "bias_test = torch.load('allsides_bias_test.pt')\n",
    "\n",
    "if remove_duplicates:\n",
    "    text_train = torch.load('allsides_duplicates_removed_contents_text_train.pt') \n",
    "    mask_train = torch.load('allsides_duplicates_removed_contents_mask_train.pt') \n",
    "else:\n",
    "    text_train = torch.load('allsides_contents_text_train.pt') \n",
    "    mask_train = torch.load('allsides_contents_mask_train.pt') \n",
    "\n",
    "text_val = torch.load('allsides_contents_text_val.pt')\n",
    "mask_val = torch.load('allsides_contents_mask_val.pt')\n",
    "\n",
    "text_test = torch.load('allsides_contents_text_test.pt')\n",
    "mask_test = torch.load('allsides_contents_mask_test.pt')\n",
    "\n",
    "# change working directory back to deep learning models directory (to decrease risk of path breaks)\n",
    "os.chdir(os.path.join(repo_path, 'deep_learning_models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing news aggregators, tabloids, and wrongly labeled source from tensors \n",
    "os.chdir(os.path.join(repo_path, 'data_preparation','allsides_data'))\n",
    "\n",
    "allsides_source_train = np.load('allsides_source_train.npy', allow_pickle=True).flatten()\n",
    "allsides_source_val = np.load('allsides_source_val.npy', allow_pickle=True).flatten()\n",
    "allsides_source_test = np.load('allsides_source_test.npy', allow_pickle=True).flatten()\n",
    "\n",
    "# sources to be removed:\n",
    "wrongly_labeled = ['RightWingWatch']\n",
    "news_aggregators = ['Drudge Report', 'Real Clear Politics', 'Yahoo News'] \n",
    "tabloids = ['New York Daily News', 'Daily Mail', 'New York Post']\n",
    "\n",
    "### choosing sources to remove \n",
    "unwanted_sources = wrongly_labeled \n",
    "if remove_aggregators:\n",
    "    unwanted_sources += news_aggregators\n",
    "if remove_tabloids:\n",
    "    unwanted_sources += tabloids\n",
    "\n",
    "# creating boolean array to mark unwanted sources\n",
    "boolean_array_train = np.full((len(allsides_source_train), ), False)\n",
    "boolean_array_val = np.full((len(allsides_source_val), ), False)\n",
    "boolean_array_test = np.full((len(allsides_source_test), ), False)\n",
    "\n",
    "for source in unwanted_sources:\n",
    "    boolean_array_train += allsides_source_train==source\n",
    "    boolean_array_val += allsides_source_val==source \n",
    "    boolean_array_test += allsides_source_test==source \n",
    "# boolean to remove aggregators\n",
    "inverted_boolean_array_train = np.invert(boolean_array_train)\n",
    "inverted_boolean_array_val = np.invert(boolean_array_val)\n",
    "inverted_boolean_array_test = np.invert(boolean_array_test)\n",
    "\n",
    "# bias\n",
    "bias_train = bias_train[inverted_boolean_array_train]\n",
    "bias_val = bias_val[inverted_boolean_array_val]\n",
    "bias_test = bias_test[inverted_boolean_array_test]\n",
    "\n",
    "# text and masks\n",
    "text_train = text_train[inverted_boolean_array_train]\n",
    "text_val = text_val[inverted_boolean_array_val]\n",
    "text_test = text_test[inverted_boolean_array_test]\n",
    "mask_train = mask_train[inverted_boolean_array_train]\n",
    "mask_val = mask_val[inverted_boolean_array_val]\n",
    "mask_test = mask_test[inverted_boolean_array_test]\n",
    "\n",
    "# sources\n",
    "allsides_source_train = allsides_source_train[inverted_boolean_array_train]\n",
    "allsides_source_val = allsides_source_val[inverted_boolean_array_val]\n",
    "allsides_source_test = allsides_source_test[inverted_boolean_array_test]\n",
    "\n",
    "os.chdir(os.path.join(repo_path, 'deep_learning_models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing one source per category for robustness testing\n",
    "    \n",
    "# sources to be removed:\n",
    "large_group = ['Reuters', 'ABC News', 'Fox News', 'Politicus USA', 'CNS News']\n",
    "small_group = ['FiveThirtyEight', 'Washington Monthly', 'The Washington Examiner', \n",
    "               'Daily Kos', 'FrontPage Magazine']\n",
    "\n",
    "if (remove_source_group == 'large') :\n",
    "    group_removed = large_group\n",
    "elif remove_source_group == 'small':\n",
    "    group_removed = small_group\n",
    "else:\n",
    "    group_removed = None\n",
    "\n",
    "if group_removed:\n",
    "    # creating boolean array to mark unwanted sources\n",
    "    boolean_array_train = np.full((len(allsides_source_train), ), False)\n",
    "\n",
    "    for source in group_removed:\n",
    "        boolean_array_train += allsides_source_train==source\n",
    "\n",
    "    # boolean to remove aggregators\n",
    "    inverted_boolean_array_train = np.invert(boolean_array_train)\n",
    "\n",
    "    # bias\n",
    "    bias_train = bias_train[inverted_boolean_array_train]\n",
    "\n",
    "    # text and masks\n",
    "    text_train = text_train[inverted_boolean_array_train]\n",
    "    mask_train = mask_train[inverted_boolean_array_train]\n",
    "\n",
    "    # sources\n",
    "    allsides_source_train = allsides_source_train[inverted_boolean_array_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(text_train, mask_train, bias_train)\n",
    "val_set = TensorDataset(text_val, mask_val, bias_val)\n",
    "test_set = TensorDataset(text_test, mask_test, bias_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, droput_prob, bert_model_module, output_attentions=False, pooled_output = True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.output_attentions = output_attentions\n",
    "        self.pooled_output = pooled_output\n",
    "\n",
    "        self.bert = bert_model_module\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.linear = nn.Linear(hidden_size,hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.classifier_layer = nn.Linear(hidden_size, num_labels) # The values are initialized from U(−sqrt(k),sqrt(k)), where k=1/in_features\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        # token_type_ids and position_ids are created automaticly \n",
    "        bert_out = self.bert(input_ids = text, attention_mask = mask)\n",
    "        \n",
    "        if self.pooled_output:\n",
    "            ### Pooled Output\n",
    "            # Choosing only CLS token output and apply linear layer + TanH \n",
    "            pooled_out = bert_out[1]\n",
    "            # Applying dropout\n",
    "            pooled_out = self.dropout(pooled_out)\n",
    "\n",
    "            out = self.classifier_layer(pooled_out)\n",
    "        else:\n",
    "            ### Last Layer average\n",
    "            # summing up over sequence lenght and devide by unmasked sequence length \n",
    "            # resulting in tensor with shape (batch_size,hidden_size)\n",
    "            last_layer = torch.sum(bert_out[0], dim=1)/torch.sum(mask,dim=1).reshape([len(mask),1])\n",
    "            last_layer = self.tanh(self.linear(last_layer))\n",
    "            last_layer = self.dropout(last_layer)\n",
    "            out = self.classifier_layer(last_layer)\n",
    "               \n",
    "        # Saving attention layer outputs if set True\n",
    "        if self.output_attentions:\n",
    "            out = out, bert_out[2]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function for training of 1 epoch\n",
    "\n",
    "def train_fct(train_set, batch_size, return_mse=False, batch_feedback = 1000, first_check = 100, mixed_precision = False, save_memory_usage = False):\n",
    "    start_time = time.time()\n",
    "    # Setting model to train mode (so dropout is applied)\n",
    "    model.train()\n",
    "    # creating iterable dataset devided into batches and shuffled\n",
    "    data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # tracking batches, loss, accuracy\n",
    "    total_batch_count = int(len(train_set)/batch_size)\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correctly_specified = 0\n",
    "    train_predicted_values = []\n",
    "    train_true_values = []\n",
    "    \n",
    "    # Tracking memory usage\n",
    "    if save_memory_usage:\n",
    "        ! nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -f memory_usage.csv \n",
    "\n",
    "    # looping over batches\n",
    "    for text, mask, label in data:\n",
    "        # sending tensors to GPU\n",
    "        text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "        # clearing gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(text, mask)\n",
    "        # calculating loss\n",
    "        loss = loss_fct(logits, label)\n",
    "\n",
    "        predictions = logits.argmax(1)\n",
    "        # backpropagation\n",
    "        if mixed_precision:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        # updating weights\n",
    "        optimizer.step()\n",
    "        # loss and metrices messures\n",
    "        train_loss += loss.item()\n",
    "        train_correctly_specified += (predictions == label).sum().item()\n",
    "        \n",
    "        train_predicted_values.append(predictions)\n",
    "        train_true_values.append(label)\n",
    "        \n",
    "        batch_counter += 1\n",
    "\n",
    "        if (batch_counter % batch_feedback == 0) or (batch_counter == first_check):\n",
    "            time_so_far = time.time() - start_time\n",
    "            minutes = int(time_so_far // 60)\n",
    "            seconds = int(time_so_far % 60)\n",
    "            average_progress_loss = train_loss/batch_counter\n",
    "            progress_acc = train_correctly_specified/(batch_counter*batch_size)\n",
    "            print('-------------------------------------------')\n",
    "            print(f'{batch_counter:5} of {total_batch_count:5} batches done after {minutes:3} min {seconds:2} sec')\n",
    "            print('-------------------------------------------')\n",
    "            print(f'loss: {average_progress_loss:6.4}   |   acc: {progress_acc:6.4}')\n",
    "            print('-------------------------------------------')\n",
    "            if save_memory_usage:\n",
    "                ! nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits >> memory_usage.csv\n",
    "            \n",
    "    # loss\n",
    "    average_total_loss = train_loss/(len(train_set)/batch_size)\n",
    "    # accuracy\n",
    "    total_accuracy = train_correctly_specified/len(train_set) \n",
    "    # Predicted and true values\n",
    "    train_predicted_values = torch.cat(train_predicted_values).cpu().numpy()\n",
    "    train_true_values = torch.cat(train_true_values).cpu().numpy()\n",
    "    # Precision\n",
    "    train_precision = precision_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    train_recall = recall_score(train_true_values, train_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    train_f1_score = f1_score(train_true_values, train_predicted_values, average='macro')\n",
    "    if return_mse:\n",
    "        train_mse = mean_squared_error(train_true_values,train_predicted_values)\n",
    "    else: \n",
    "        train_mse = None\n",
    "    \n",
    "    # Loading memory usage to get maxium\n",
    "    if save_memory_usage:\n",
    "        memory_usage = np.loadtxt('memory_usage.csv', dtype='int', delimiter = ',') # csv-file name\n",
    "        max_memory_usage = int(np.max(memory_usage))\n",
    "    else:\n",
    "        max_memory_usage = None\n",
    "    \n",
    "    return average_total_loss, total_accuracy, train_precision, train_recall, train_f1_score, train_mse, max_memory_usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function for validation after 1 epoch of training\n",
    "\n",
    "def val_fct(val_set, batch_size, return_mse=False, return_predicted_values=False):\n",
    "    print('----------- Start Validation/Testing ----------')\n",
    "    # Setting model to evaluation mode (dropout is not applied)\n",
    "    model.eval()\n",
    "    # creating iterable dataset devided into batches, not shuffeled\n",
    "    data = DataLoader(val_set, batch_size = batch_size)\n",
    "    # setting up loss and accuracy variables\n",
    "    val_loss = 0\n",
    "    #val_correctly_specified = 0\n",
    "    val_predicted_values = []\n",
    "    val_true_values = []\n",
    "    # looping over batches\n",
    "    for text, mask, label in data:\n",
    "        text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "        # no gradient calculation during validation\n",
    "        with torch.no_grad():\n",
    "            logits = model(text, mask)\n",
    "            # calculating loss\n",
    "            loss = loss_fct(logits, label)            \n",
    "            predictions = logits.argmax(1)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_predicted_values.append(predictions)\n",
    "            val_true_values.append(label)\n",
    "    \n",
    "    # loss\n",
    "    average_val_loss = val_loss/(len(val_set)/batch_size)\n",
    "    # true and predicted values\n",
    "    val_predicted_values = torch.cat(val_predicted_values).cpu().numpy()\n",
    "    val_true_values = torch.cat(val_true_values).cpu().numpy()\n",
    "    # Accuracy\n",
    "    val_accuracy = (val_predicted_values==val_true_values).sum().item()/len(val_set) \n",
    "    # Precision\n",
    "    val_precision = precision_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Recall\n",
    "    val_recall = recall_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # F1 score\n",
    "    val_f1_score = f1_score(val_true_values, val_predicted_values, average='macro')\n",
    "    # Mean squared error\n",
    "    if return_mse:\n",
    "        val_mse = mean_squared_error(val_true_values,val_predicted_values)\n",
    "    else:\n",
    "        val_mse = None\n",
    "    \n",
    "    if not return_predicted_values:\n",
    "        val_predicted_values = None\n",
    "\n",
    "    return average_val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_mse, val_predicted_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "    return sum(layer.numel() for layer in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cost_matrix(num_labels):\n",
    "    '''\n",
    "    Creates cost matrix with shape (num_labels,num_labels) that gives higher \n",
    "    weights the further away a prediction is from the true value. \n",
    "    Weights are calculated with: 0.2 + 0.2*difference_in_lable_values\n",
    "    '''\n",
    "    label_columns = torch.arange(num_labels)\n",
    "    cost_matrix = torch.zeros((num_labels,num_labels))\n",
    "    for i,row in enumerate(cost_matrix):\n",
    "        cost_matrix[i] = torch.abs(label_columns-i) * 0.2 + 0.2\n",
    "    return cost_matrix\n",
    "\n",
    "def cost_sensitive_log_softmax(x, target, cost_matrix): \n",
    "    '''\n",
    "    Weighting as in Khan (2017) under III.C.(c) Cost-Sensitive CE loss, equation (11)\n",
    "    '''\n",
    "    # numerator of softmax: multiplied with weight from cost matrix (cost_matrix[predicted_label,target_label])\n",
    "    weights_predicted = cost_matrix[x.argmax(-1),target].reshape(-1,1).detach()\n",
    "    numerator = x*weights_predicted\n",
    "    # denominator of softmax: each value in sum multiplied with coresponding weights \n",
    "    # (cost_matrix[predicted_label,:])\n",
    "    weights_all = cost_matrix[x.argmax(-1),:].detach()\n",
    "    denominator = x*weights_all\n",
    "    # after taking log:\n",
    "    return numerator - denominator.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "class CostSensitiveCELoss(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.cost_matrix = create_cost_matrix(num_labels).to(device)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        weighted_log_softmax_out = cost_sensitive_log_softmax(input, target, self.cost_matrix) \n",
    "        return F.nll_loss(weighted_log_softmax_out,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading Bert \n",
    "BertModel = transformers.BertModel\n",
    "\n",
    "### Device to run model on, either GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Model inputs\n",
    "hidden_size = 768\n",
    "num_labels = 5 \n",
    "dropout_prob = 0.1\n",
    "\n",
    "### Hyperparameters\n",
    "batch_size = 16 \n",
    "learning_rate = 2e-5\n",
    "### Use of nvidia apex for mixed precession calculations\n",
    "mixed_precision = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "##### Initilize and configure Bert\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "\n",
    "##### Initilize model \n",
    "model = Model(hidden_size, num_labels, dropout_prob, bert_model, pooled_output=True).to(device)\n",
    "\n",
    "### Optimizer, choosing learning rate \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "### Applying mixed precision to speed up model training \n",
    "if mixed_precision:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") \n",
    "### Loss function\n",
    "if cost_sensitive:\n",
    "    loss_fct = CostSensitiveCELoss(num_labels).to(device)\n",
    "else: \n",
    "    loss_fct = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 110,076,677\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters: {num_parameters(model):,}' )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_fct(deviation_case, num_epochs, seed):\n",
    "    '''\n",
    "    Function to train model for a given number of epochs and saving all necessery figures and model weights\n",
    "    '''\n",
    "   \n",
    "    ### Dictionary to save metrices\n",
    "    metric_scores = {'epoch': [], 'time': [], \n",
    "                     'train_loss': [], 'train_acc': [], 'train_precision': [], 'train_recall': [], 'train_f1_score': [], 'train_mse': [],\n",
    "                     'val_loss': [], 'val_acc': [], 'val_precision': [], 'val_recall': [], 'val_f1_score': [], 'val_mse': [],\n",
    "                     'test_loss': [], 'test_acc': [], 'test_precision': [], 'test_recall': [], 'test_f1_score': [], 'test_mse': [], 'memory': []}\n",
    "    \n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training for 1 epoch\n",
    "        train_loss, train_acc, train_precision, train_recall, \\\n",
    "        train_f1_score, train_mse, max_memory_usage = train_fct(train_set, \n",
    "                                                                batch_size, \n",
    "                                                                return_mse=True, \n",
    "                                                                batch_feedback=5000, \n",
    "                                                                first_check=100,\n",
    "                                                                mixed_precision = mixed_precision, \n",
    "                                                                save_memory_usage = True)\n",
    "        # Validation\n",
    "        val_loss, val_acc, val_precision, val_recall, \\\n",
    "        val_f1_score, val_mse, val_predicted_values = val_fct(val_set, \n",
    "                                                              batch_size, \n",
    "                                                              return_mse=True, \n",
    "                                                              return_predicted_values=True)\n",
    "\n",
    "        test_loss, test_acc, test_precision, test_recall, \\\n",
    "        test_f1_score, test_mse, test_predicted_values = val_fct(test_set, \n",
    "                                                                 batch_size, \n",
    "                                                                 return_mse=True, \n",
    "                                                                 return_predicted_values=True)\n",
    "\n",
    "        # Display results\n",
    "        end_time = time.time() - epoch_start_time\n",
    "        minutes = int(end_time // 60)\n",
    "        seconds = int(end_time % 60)\n",
    "        print('+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +' + (' + + + + + + +' if train_mse else ''))\n",
    "        print(f'+ Epoch: {epoch} took {minutes:3} min, {seconds:2} sec                             ')\n",
    "        try:\n",
    "            print(f'+ Maximum memory usage: {max_memory_usage:5} MiB')\n",
    "        except TypeError:\n",
    "            pass\n",
    "        print(f'+ (Training)   Loss: {train_loss:6.4}  |  Acc: {train_acc:6.4}  |  F1: {train_f1_score:6.4}  ' + (f'|  MSE: {train_mse:.4}' if train_mse else ''))\n",
    "        print(f'+ (Validation) Loss: {val_loss:6.4}  |  Acc: {val_acc:6.4}  |  F1: {val_f1_score:6.4}  ' + (f'|  MSE: {val_mse:.4}' if val_mse else ''))\n",
    "        print('+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +' + (' + + + + + + +' if train_mse else ''))    \n",
    "\n",
    "        # saving metrices\n",
    "        current_epoch_score_metrics = ['epoch', 'time', \n",
    "                                       'train_loss', 'train_acc', 'train_precision', 'train_recall', 'train_f1_score', 'train_mse',\n",
    "                                       'val_loss', 'val_acc', 'val_precision', 'val_recall', 'val_f1_score', 'val_mse',\n",
    "                                       'test_loss', 'test_acc', 'test_precision', 'test_recall', 'test_f1_score', 'test_mse', 'memory']\n",
    "        current_epoch_score_values = [epoch, round(end_time/60,2), \n",
    "                                      train_loss, train_acc, train_precision, train_recall, train_f1_score, train_mse,\n",
    "                                      val_loss, val_acc, val_precision, val_recall, val_f1_score, val_mse,\n",
    "                                      test_loss, test_acc, test_precision, test_recall, test_f1_score, test_mse, \n",
    "                                      max_memory_usage]\n",
    "        for metric,value in zip(current_epoch_score_metrics, current_epoch_score_values):\n",
    "            metric_scores[metric].append(value)\n",
    "        \n",
    "        # saving model weights \n",
    "        if mixed_precision:\n",
    "            checkpoint = {'model': model.state_dict(),\n",
    "                          'optimizer': optimizer.state_dict(),\n",
    "                          'amp': amp.state_dict()}\n",
    "\n",
    "            torch.save(checkpoint, f'weights/amp_checkpoint_{deviation_case}_epoch{epoch}.pt')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f'weights/model_weights_{deviation_case}_epoch{epoch}.pt')\n",
    "\n",
    "        # saving final scores\n",
    "        if epoch==num_epochs:\n",
    "            results = pd.DataFrame(metric_scores)\n",
    "            results.to_csv(f'scores/metric_scores_{deviation_case}.csv', index=False)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the GitHub Discussion regarding gradient overflow: \"Occasionally seeing a message like “overflow detected, skipping step, reducing loss scale” is normal behavior with dynamic loss scaling, and it usually happens in the first few iterations because Amp begins by trying a high loss scale.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerunning model to decrease variance due to randomness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "  100 of 12345 batches done after   1 min 16 sec\n",
      "-------------------------------------------\n",
      "loss:  1.528   |   acc: 0.3275\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "-------------------------------------------\n",
      " 5000 of 12345 batches done after  63 min 31 sec\n",
      "-------------------------------------------\n",
      "loss: 0.7675   |   acc: 0.7234\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "-------------------------------------------\n",
      "10000 of 12345 batches done after 127 min  2 sec\n",
      "-------------------------------------------\n",
      "loss: 0.6345   |   acc: 0.7721\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "----------- Start Validation/Testing ----------\n",
      "----------- Start Validation/Testing ----------\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "+ Epoch: 1 took 171 min,  1 sec                             \n",
      "+ Maximum memory usage: 12508 MiB\n",
      "+ (Training)   Loss: 0.5994  |  Acc: 0.7858  |  F1: 0.7838  |  MSE: 1.22\n",
      "+ (Validation) Loss: 0.4701  |  Acc: 0.8315  |  F1: 0.8296  |  MSE: 0.9104\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "-------------------------------------------\n",
      "  100 of 12345 batches done after   1 min 16 sec\n",
      "-------------------------------------------\n",
      "loss: 0.3832   |   acc: 0.8606\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "-------------------------------------------\n",
      " 5000 of 12345 batches done after  63 min 27 sec\n",
      "-------------------------------------------\n",
      "loss: 0.3723   |   acc: 0.8721\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "-------------------------------------------\n",
      "10000 of 12345 batches done after 126 min 55 sec\n",
      "-------------------------------------------\n",
      "loss: 0.3638   |   acc: 0.8754\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "----------- Start Validation/Testing ----------\n",
      "----------- Start Validation/Testing ----------\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "+ Epoch: 2 took 170 min, 53 sec                             \n",
      "+ Maximum memory usage: 13062 MiB\n",
      "+ (Training)   Loss: 0.3599  |  Acc: 0.8767  |  F1: 0.8756  |  MSE: 0.6607\n",
      "+ (Validation) Loss: 0.3892  |  Acc: 0.8689  |  F1: 0.8683  |  MSE: 0.7803\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "-------------------------------------------\n",
      "  100 of 12345 batches done after   1 min 16 sec\n",
      "-------------------------------------------\n",
      "loss: 0.2578   |   acc: 0.9175\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "-------------------------------------------\n",
      " 5000 of 12345 batches done after  63 min 23 sec\n",
      "-------------------------------------------\n",
      "loss: 0.2607   |   acc: 0.9131\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "10000 of 12345 batches done after 126 min 44 sec\n",
      "-------------------------------------------\n",
      "loss: 0.2636   |   acc: 0.9116\n",
      "-------------------------------------------\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "----------- Start Validation/Testing ----------\n",
      "----------- Start Validation/Testing ----------\n"
     ]
    }
   ],
   "source": [
    "### #RUN ###\n",
    "run = 2  ###\n",
    "############\n",
    "num_epochs = 3\n",
    "### name experiment\n",
    "deviation_case = f'allsides_full_rerun_{run}'\n",
    "\n",
    "seed = 19 + run # =20/21/22\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model_training_fct(deviation_case, num_epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopping instance\n",
    "! gcloud compute instances stop t4-instance --zone=europe-west4-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
