{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on Semantic Evaluation 2019 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### choose model type #########\n",
    "model_type = 'bilstm' #'bert' # \n",
    "###############################\n",
    "\n",
    "### choose weights ##############################################\n",
    "file_name =  'dl_benchmark_allsides_all_removed'                #\n",
    "# 'allsides_aggregators_tabloids_duplicates_removed'            #\n",
    "# 'allsides_full'                                               #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "bias_semeval = torch.load('semeval_bias_tensor.pt')\n",
    "text_semeval = torch.load('semeval_contents_text_tensor.pt')\n",
    "mask_semeval = torch.load('semeval_contents_mask_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'bert':\n",
    "    semeval_set = TensorDataset(text_semeval, mask_semeval, bias_semeval)\n",
    "else:\n",
    "    semeval_set = TensorDataset(text_semeval, bias_semeval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BiLSTM subclasses/functions\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention' from 'Attention is all you need'\n",
    "    \"\"\"  \n",
    "    hidden_size = query.size(-1)\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(hidden_size)\n",
    "    # if mask is not None:\n",
    "    #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attention_weights = F.softmax(attention_scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    return torch.matmul(attention_weights, value), attention_weights\n",
    "\n",
    "class LSTMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds block consisting of an LSTM and a fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout_prob, apply_attention=False, last_layer=False, bidirectional=True):\n",
    "        \"\"\"\n",
    "        hidden_size: Number of hidden neurons in LSTM and number of feature inputs (size of embedding/size of \n",
    "                     hidden layer output in stacked LSTM )\n",
    "        last_layer: Indicates whether to add \"boom\" layer (False) or not (True)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.directions = 1 + bidirectional\n",
    "        self.last_layer = last_layer\n",
    "        self.apply_attention = apply_attention\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        self.layer_norm_input = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm_lstm = nn.LayerNorm(self.directions*hidden_size)\n",
    "        \n",
    "        if apply_attention:\n",
    "            self.layer_norm_querry = nn.LayerNorm(self.directions*hidden_size, eps=1e-12)\n",
    "            self.layer_norm_key = nn.LayerNorm(self.directions*hidden_size)\n",
    "            self.layer_norm_value = nn.LayerNorm(self.directions*hidden_size)\n",
    "\n",
    "            self.qs = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "            self.ks = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "            self.vs = nn.Parameter(torch.zeros(size=(1, 1, self.directions*hidden_size), dtype=torch.float))\n",
    "\n",
    "            self.querry_projection = nn.Linear(self.directions*hidden_size,self.directions*hidden_size)\n",
    "            self.linear_over_param = nn.Linear(self.directions*hidden_size, 2*self.directions*hidden_size) \n",
    "\n",
    "        if not last_layer: \n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.boom_a = nn.Linear(hidden_size*self.directions,4*self.directions*hidden_size) \n",
    "            self.activation = nn.GELU() \n",
    "            self.boom_b = nn.Linear(4*self.directions*hidden_size, hidden_size) \n",
    "    \n",
    "    def forward(self, feature_input):\n",
    "        ### Adjust batch size in case of last batch being shorter \n",
    "        current_batch_size = feature_input.shape[0]\n",
    "        # Initilize hidden and cell state\n",
    "        hidden_0 = torch.zeros(self.directions, current_batch_size, self.hidden_size).to(device)\n",
    "        # Applying layer normalization to input\n",
    "        feature_input = self.layer_norm_input(feature_input)\n",
    "        # LSTM layer with embeddings/hidden-state inputs \n",
    "        lstm_out, (last_hidden,last_cell) = self.lstm(feature_input, (hidden_0,hidden_0))\n",
    "\n",
    "        if self.apply_attention:\n",
    "            # Taken from Merity (2019):\n",
    "            # matrix multiplication and layer normalization on querry\n",
    "            querry = self.layer_norm_querry(self.querry_projection(lstm_out))\n",
    "            # only layer normalization on key and value\n",
    "            key = self.layer_norm_key(lstm_out)\n",
    "            value = self.layer_norm_value(lstm_out)\n",
    "            # activation of parameter vectors\n",
    "            qs, ks, vs = torch.sigmoid(self.qs), torch.sigmoid(self.ks), torch.sigmoid(self.vs) \n",
    "            # over parameterizing of value parameter vector, using forget gate and candidate (Merity 2019, 6.4)\n",
    "            candidate, forget = self.linear_over_param(vs).split(self.directions*self.hidden_size, dim=-1) \n",
    "            vs = torch.sigmoid(forget) * torch.tanh(candidate) \n",
    "            # multiplaying parameter vectors with querry, key, and value respectively\n",
    "            q, k, v, = qs*querry, ks*key, vs*value \n",
    "            # apply scaled dot product attention\n",
    "            lstm_out, attention_weights = attention(q,k,v, dropout=self.dropout)\n",
    "\n",
    "        # Applying layer normalization to lstm output\n",
    "        lstm_out = self.layer_norm_lstm(lstm_out)\n",
    "\n",
    "        if self.last_layer:\n",
    "            return lstm_out, last_hidden\n",
    "        else:\n",
    "            # big fully connected layer taking shape(batch, seq_len, num_directions * hidden_size) \n",
    "            # and returning shape(batch, seq_len, hidden_size)\n",
    "            boom_out = self.boom_b(self.dropout(self.activation(self.boom_a(lstm_out))))\n",
    "            return boom_out\n",
    "\n",
    "### Choosing weither bert or bilstm is model used\n",
    "if model_type == 'bert':\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, hidden_size, num_labels, droput_prob, bert_model_module, output_attentions=False, pooled_output = True):\n",
    "            super().__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_labels = num_labels\n",
    "            self.output_attentions = output_attentions\n",
    "            self.pooled_output = pooled_output\n",
    "\n",
    "            self.bert = bert_model_module\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.linear = nn.Linear(hidden_size,hidden_size)\n",
    "            self.tanh = nn.Tanh()\n",
    "\n",
    "            self.classifier_layer = nn.Linear(hidden_size, num_labels) # The values are initialized from U(âˆ’sqrt(k),sqrt(k)), where k=1/in_features\n",
    "\n",
    "        def forward(self, text, mask):\n",
    "            # token_type_ids and position_ids are created automaticly \n",
    "            bert_out = self.bert(input_ids = text, attention_mask = mask)\n",
    "\n",
    "            if self.pooled_output:\n",
    "                ### Pooled Output\n",
    "                # Choosing only CLS token output and apply linear layer + TanH \n",
    "                pooled_out = bert_out[1]\n",
    "                # Applying dropout\n",
    "                pooled_out = self.dropout(pooled_out)\n",
    "\n",
    "                out = self.classifier_layer(pooled_out)\n",
    "            else:\n",
    "                ### Last Layer average\n",
    "                # summing up over sequence lenght and devide by unmasked sequence length \n",
    "                # resulting in tensor with shape (batch_size,hidden_size)\n",
    "                last_layer = torch.sum(bert_out[0], dim=1)/torch.sum(mask,dim=1).reshape([len(mask),1])\n",
    "                last_layer = self.tanh(self.linear(last_layer))\n",
    "                last_layer = self.dropout(last_layer)\n",
    "                out = self.classifier_layer(last_layer)\n",
    "\n",
    "            # Saving attention layer outputs if set True\n",
    "            if self.output_attentions:\n",
    "                out = out, bert_out[2]\n",
    "\n",
    "            return out\n",
    "        \n",
    "else:\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, seq_length, hidden_size, num_labels, num_layers, \n",
    "                     vocabulary_size, dropout_prob = 0.1, bidirectional = True, attention_layer=False):\n",
    "            \"\"\"\n",
    "            seq_length: Length of input sequence (Text) NOT USED\n",
    "            hidden_size (==embedding_size): Number or hidden neurons in LSTM, also used for embedding size\n",
    "            num_labels: Number of target labels\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_labels = num_labels\n",
    "            self.num_layers = num_layers\n",
    "            self.directions = 1 + bidirectional\n",
    "            self.attention_layer = attention_layer\n",
    "\n",
    "            self.embedding = nn.Embedding(vocabulary_size, hidden_size, padding_idx=0)\n",
    "\n",
    "            self.blocks = nn.ModuleList()\n",
    "            for i in range(num_layers):\n",
    "                # last layer\n",
    "                if i==num_layers-1:\n",
    "                    self.blocks.append(LSTMBlock(hidden_size, dropout_prob=0, last_layer=True)) \n",
    "                # second last layer with attention\n",
    "                elif i==num_layers-2:\n",
    "                    self.blocks.append(LSTMBlock(hidden_size, dropout_prob=dropout_prob, apply_attention=self.attention_layer))\n",
    "                # other layers\n",
    "                else:\n",
    "                    self.blocks.append(LSTMBlock(hidden_size, dropout_prob=dropout_prob)) \n",
    "\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.classifier_a = nn.Linear(hidden_size*self.directions,4*self.directions*hidden_size) \n",
    "            self.activation = nn.GELU() # nn.Tanh()\n",
    "            self.classifier_b = nn.Linear(4*self.directions*hidden_size, hidden_size) \n",
    "            self.classifier_c = nn.Linear(hidden_size, num_labels) \n",
    "\n",
    "        def forward(self, text):\n",
    "            ### Embeddings\n",
    "            embeddings = self.embedding(text)\n",
    "\n",
    "            ### LSTM + \"Boom\"-layer blocks\n",
    "            for i,block in enumerate(self.blocks):\n",
    "                # only single layer\n",
    "                if len(self.blocks)==1:\n",
    "                    last_hidden = block(embeddings)\n",
    "                # first layer\n",
    "                elif i==0:\n",
    "                    block_out = block(embeddings)\n",
    "                # last layer\n",
    "                elif i==len(self.blocks)-1:\n",
    "                    lstm_out, last_hidden = block(block_out)\n",
    "                # other layers\n",
    "                else:\n",
    "                    block_out = block(block_out)\n",
    "\n",
    "            if self.directions==2:\n",
    "                # adjust last hidden state output to shape (batch_size,directions*hidden_size), i.e. concatinating both directions\n",
    "                last_hidden = torch.cat((last_hidden[0,:,:],last_hidden[1,:,:]), axis=1) \n",
    "\n",
    "            ### Classifier layer\n",
    "            output = self.classifier_b(self.dropout(self.activation(self.classifier_a(last_hidden))))\n",
    "            output = self.classifier_c(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'bert':\n",
    "    BertModel = transformers.BertModel\n",
    "\n",
    "    ### Device to run model on, either GPU or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### Model inputs\n",
    "    hidden_size = 768\n",
    "    num_labels = 5 \n",
    "    dropout_prob = 0.1\n",
    "    cost_sensitive = False\n",
    "\n",
    "    ### Hyperparameters\n",
    "    batch_size = 16 \n",
    "    learning_rate = 2e-5\n",
    "\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "    model = Model(hidden_size, num_labels, dropout_prob, bert_model, pooled_output=True).to(device)\n",
    "else:\n",
    "    ### Device to run model on, either GPU or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### Model inputs\n",
    "    seq_length = 500 \n",
    "    hidden_size = 512\n",
    "    num_labels = 5\n",
    "    num_layers = 4\n",
    "    vocabulary_size = 30522\n",
    "    dropout_prob = 0.1\n",
    "    bidirectional = True\n",
    "    attention_layer = True\n",
    "    ### Hyperparameters\n",
    "    batch_size = 64\n",
    "    \n",
    "    model = Model(seq_length, hidden_size, num_labels, num_layers, vocabulary_size, \n",
    "                  dropout_prob, bidirectional, attention_layer).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on SemEval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Took 2e+01 seconds\n",
      "Accuracy: 0.6698   F1-Score: 0.5535\n",
      "Done. Took 2e+01 seconds\n",
      "Accuracy: 0.6574   F1-Score: 0.5641\n",
      "Done. Took 2e+01 seconds\n",
      "Accuracy: 0.6946   F1-Score: 0.6175\n"
     ]
    }
   ],
   "source": [
    "results_per_run = np.zeros((2,3))\n",
    "\n",
    "for run in range(1,4):\n",
    "    if model_type == 'bert':\n",
    "        checkpoint = torch.load(os.path.join(repo_path,'deep_learning_models' , 'weights', f'amp_checkpoint_{file_name}_rerun_{run}_epoch3.pt'))\n",
    "    else:\n",
    "        torch.load(os.path.join(repo_path, 'deep_learning_models', 'dl_benchmark_weights', f'amp_checkpoint_{file_name}_rerun_{run}_epoch5.pt'))\n",
    "\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    ### Get predicted values \n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = DataLoader(semeval_set, batch_size=batch_size)\n",
    "    test_loss = 0\n",
    "    test_predicted_values = []\n",
    "\n",
    "    batch_counter = 0\n",
    "    \n",
    "    if model_type == 'bert':\n",
    "        for text, mask, label in data:\n",
    "            text, mask, label = text.to(device), mask.to(device), label.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(text,mask)\n",
    "\n",
    "                test_predicted_values.append(output.argmax(1))\n",
    "    else:\n",
    "        for text, label in data:\n",
    "            text, label = text.to(device), label.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(text)\n",
    "\n",
    "                test_predicted_values.append(output.argmax(1))        \n",
    "                \n",
    "    print(f'Done. Took {(time.time()-start_time):.1} seconds')\n",
    "    \n",
    "    test_predicted_values = torch.cat(test_predicted_values).cpu().numpy()\n",
    "    \n",
    "    # converting allsides labels (5) to semeval labels (2)\n",
    "    predicted_values_semeval_labels = np.zeros(test_predicted_values.shape)\n",
    "\n",
    "    predicted_values_semeval_labels[test_predicted_values==0] = 1\n",
    "    predicted_values_semeval_labels[test_predicted_values==1] = 0\n",
    "    predicted_values_semeval_labels[test_predicted_values==2] = 0\n",
    "    predicted_values_semeval_labels[test_predicted_values==3] = 0\n",
    "    predicted_values_semeval_labels[test_predicted_values==4] = 1\n",
    "    \n",
    "    # accuracy and F1 score of semeval predictions \n",
    "    test_accuracy = (predicted_values_semeval_labels==bias_semeval.numpy()).sum()/len(predicted_values_semeval_labels) \n",
    "    test_f1_score = f1_score(bias_semeval.numpy(), predicted_values_semeval_labels)\n",
    "    print(f'Accuracy: {test_accuracy:.4}   F1-Score: {test_f1_score:.4}')\n",
    "    results_per_run[0,run-1] = test_accuracy\n",
    "    results_per_run[1,run-1] = test_f1_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.6739 and F1: 0.5783 of allsides_aggregators_tabloids_duplicates_removed\n"
     ]
    }
   ],
   "source": [
    "average_results = np.mean(results_per_run, axis=1)\n",
    "print(f'Acc: {average_results[0]:.4} and F1: {average_results[1]:.4} of {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4946   F1-Score: 0.3122\n",
      "Accuracy: 0.5178   F1-Score: 0.3561\n",
      "Accuracy: 0.5349   F1-Score: 0.3534\n",
      "Acc: 0.5158 and F1: 0.3406 of random runs\n"
     ]
    }
   ],
   "source": [
    "# creating random predictions according to true class distribution\n",
    "prob_non_partisan = np.unique(bias_semeval.cpu().numpy(), return_counts=True)[1][0]/len(bias_semeval)\n",
    "prob_hyperpartisan = np.unique(bias_semeval.cpu().numpy(), return_counts=True)[1][1]/len(bias_semeval)\n",
    "\n",
    "random_results_per_run = np.zeros((2,3))\n",
    "\n",
    "for i in range(3):\n",
    "    random_predictions = np.random.choice([0,1], size=len(bias_semeval), p=[prob_non_partisan, prob_hyperpartisan])\n",
    "    # get results for random predictions \n",
    "    random_accuracy = (random_predictions==bias_semeval.numpy()).sum()/len(predicted_values_semeval_labels) \n",
    "    random_f1_score = f1_score(bias_semeval.numpy(), random_predictions)\n",
    "    print(f'Accuracy: {random_accuracy:.4}   F1-Score: {random_f1_score:.4}')\n",
    "    random_results_per_run[0, i] = random_accuracy\n",
    "    random_results_per_run[1, i] = random_f1_score\n",
    "    \n",
    "average_random_results = np.mean(random_results_per_run, axis=1)\n",
    "print(f'Acc: {average_random_results[0]:.4} and F1: {average_random_results[1]:.4} of random runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
